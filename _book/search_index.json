[["index.html", "Demons and Decisions A Defence of Causal Decision Theory Front Matter", " Demons and Decisions A Defence of Causal Decision Theory Brian Weatherson 2022-06-25 Front Matter Draft of a book in progress defending a novel version of Causal Decision Theory. "],["intro.html", "Chapter 1 Introduction 1.1 Two Questions 1.2 Ideal Decision Theory 1.3 Proceduralism and Ratificationism 1.4 Causal Ratificationism", " Chapter 1 Introduction 1.1 Two Questions Decision theory should have at its heart two questions. What makes a decision good? How does one make good decisions? It is of the first importance that these questions not be confused. The answers to the two questions are different, and the constraints on plausible answers to the two questions are different. This book argues for a particular answer to the first question. I’ll call this answer causal ratificationism. It says that what makes a decision good is that the decision maker can rationally defend it. Once the decision is made, the decision makes sense. The decision can be ratified by the decision maker. That’s the reason for including ‘ratificationism’ in the name. The account I’ll give of what it is to rationally ratify a decision gives a central role to causal concepts, which is why I’ve included the first word in the name. Once I’ve set it out, causal ratificationism won’t look too surprising to people familiar with contemporary game theory. But it does look fairly different to most contemporary decision theory. For example, it doesn’t assign values to options, and instruct choosers to maximise value. Most contemporary work in decision theory does just that, with the primary disputes being over how to identify the values. Causal ratificationism most closely resembles work in decision theory that follows Skyrms (1990) in looking at whether a chooser’s views are in equilibrium. But most such views go on to identify a privileged equilibrium in decision problems that have multiple equilibria. Causal ratificationism does not. Very roughly, it just says that the chooser must rationally be in equilibrium, and then their choice is rational. Causal ratificationism isn’t just different from most existing theories. It violates all sorts of constraints that philosophical decision theorists have, implicitly or explicitly, imposed on theories of decision. I’ll set out these constraints precisely in section 1.3, and call them collectively proceduralism. Much of the rest of the book will be a sustained argument against proceduralism. If one were trying to answer the second question I started with, how should one make good decisions, proceduralism would be a very plausible set of constraints. But they are not plausible constraints on answers to the first question, what makes a decision good. If I’m going to rest so much argumentative weight on the difference between these questions, and on the different constraints on answers to them, I better say more about what the questions mean, and why I think they are so different. Let’s turn to that first. 1.2 Ideal Decision Theory 1.2.1 Decisions Under Certainty Most philosophical work on decision theory focusses on choices under uncertainty. And I’m going to follow suit for most of this book. But not yet. Instead, I’ll start with an example of decision making under certainty, and in particular a version of the problem that Julia Robinson dubbed the ‘travelling salesman’ problem.1 Salesman Chooser is given the straight line distance between each pair of cities from the 257 represented on the map below. Using this information, Chooser has to find as short a path as possible that goes through all 257 cities and returns to the first one. The longer a path Chooser selects, the worse things will be for Chooser. Figure 1.1: The 257 cities that must be visited in the Salesman problem. Since there are \\(256!\\) possible paths, Chooser has a few options here.2 Question: What makes a particular choice of path a good one? Answer: It’s no longer than the other \\(256!-1\\) options. Question: How should Chooser make a good choice here? Answer: Read a lot of the literature on this kind of optimisation problem, look for ways to rule out large collections of options all at once, etc. These answers are quite different, and that means the questions are different to. The first question is asking something almost metaphysical - in virtue of what is the right answer the right one? The second question is practical - how do we go about finding that right answer, or at least getting close to it? It would be really terrible to answer the second question by saying “choose the shortest route”. This wouldn’t be obviously incorrect. But it would be useless, which in context is almost as bad. The second question is asking a question in what we might call non-ideal decision theory.3 And a constraint on a non-ideal theory is that it is useful, in a way that “choose the shortest route” is useless. But this is the right answer to the first question. The shortest route is, kind of by definition in this case, the best choice. That’s the ideal. An ideal decision theory should say to choose it.4 1.2.2 Decisions Under Uncertainty It’s time to introduce uncertainty into the mix, because cases like Salesman might leave a misleading impression about what ideal decision theory tries to achieve. Basketball Chooser is at a casino, and a basketball game is about to start. Chooser knows that basketball games don’t have draws or ties; one side will win. And Chooser knows the teams are equally balanced; each team is 50% likely to win. Chooser has three options. They can bet on the Home team to win, bet on the Away team to win, or Pass, and not bet. If they bet, they win $100 if the team they bet on wins, and lose $110 if that team loses. If they Pass, they neither gain nor lose anything. Ideal decision theory says that in Basketball, Chooser should Pass. That’s not the optimal outcome for Chooser. The optimal outcome is that they bet on the winning team. But since they don’t know who that is, and either bet will, on average, lose them money, they should Pass rather than bet on Home or Away. Still, there is some sense in which they should not Pass - it doesn’t produce the best outcomes. We could have a theory that just evaluated the possible outcomes in any decision. I’ll call this Outcome Evaluation Theory, though it’s more commonly called axiology. My name has the vice of being longer, but the virtue of using more familiar and descriptive language, so I’ll stick with it. Now there are three theories on the table: Outcome Evaluation Theory, Ideal Decision Theory, and Non-Ideal Decision Theory. One initial claim I want to make is that ordinary claims about what one should do are often three-way ambiguous, with the three ways corresponding to these three theories. Sometimes the claim is that one will produce the best outcome by doing this, sometimes that Ideal Decision Theory says to do it, and sometimes that Non-Ideal Decision Theory says to do it. Possibly the middle one, about Ideal Decision Theory, is the least commonly used in ordinary thought and talk, but it is a possible interpretation of a claim that one should do a particular thing. It’s just that it’s not the only possible interpretation of that claim. We can distinguish these three theories by what they say to do in two examples introduced so far: Salesman and Basketball. Table 1.1: How three kinds of theories handle two problems. Theory Salesman Basketball Outcome Evaluation Shortest route Bet on winner Ideal Decision Shortest route Pass Non-Ideal Decision Study optimization Pass Ideal Decision Theory is an odd mid-point between the two other theories listed. It takes Chooser’s worldly knowledge as it is, and Chooser’s mathematical abilities as they might be. That is, it idealises away from mathematical shortcomings, but not worldly ignorance. One question we might well ask is why we care about just these idealisations. A related question is what benefit we gain from studying Ideal Decision Theory. Outcome Evaluation is obviously useful; we need to know what’s better than what. Non-Ideal Decision Theory is obviously useful; it’s good to make better decisions. But what do we gain from Ideal Decision Theory? The answer I’m going to give in chapter 3 is that Ideal Decision Theory is a vital input into worthwhile explanatory projects. But for now what I want to argue is that Ideal Decision Theory is distinct from Outcome Evaluation Theory and from Non-Ideal Decision Theory. And not just is it distinct, it’s the project that philosophy decision theorists have primarily been engaged in for decades. 1.2.3 Decision Theorists are Ideal Most philosophical work in decision theory concerns what I’ve called Ideal Decision Theory. At least, that’s what I’m going to argue in this subsection. And I’m going to argue for it with two caveats. First, this particular usage of the Ideal/Non-Ideal distinction is mine. So it isn’t that decision theorists are addressing Ideal Decision Theory under that label. But they are, I’ll presently argue, implicitly addressing it. Second, the reason I say ‘most’ not ‘all’ here is primarily to exclude what is called ‘descriptive decision theory’ (Chandler 2017) from the scope of the my claim. Descriptive decision theory concerns how people actually make decisions. Most of that work takes place outside philosophy, but I don’t want to get into needless border wars to defend the claim I’m making. So I’ll just note that philosophers working on descriptive decision theory are definitely not working on Ideal Decision Theory. But Non-Ideal Decision Theory isn’t Descriptive Decision Theory. It’s a normative theory; it’s about what people should do. The verdicts of Non-Ideal Decision Theory are that someone facing a problem like Salesman should learn some optimisation theory, that someone facing a novel and difficult situation should stop and have a think about their options, and so on. These are normative claims; they are about what the person should do. But they are not the kinds of claims that Outcome Evaluation Theory, or Ideal Decision Theory make. And my main sociological claim is that very few philosophers work on this normative but Non-Ideal branch of decision theory. Very few isn’t zero, and there are philosophers who aim to take into account the kinds of limitations that Non-Ideal Decision Theory accounts for. But they are a small minority of decision theorists. To see this, think about what most decision theorists say about our two examples: Salesman and Basketball. I’m not going to go over the range of what different decision theorists say case by case, for a couple of reasons. First, there are several good surveys of these views. The Stanford Encyclopedia alone has three surveys of different parts of the field (Steele and Stefánsson 2020; Weirich 2020; Briggs 2019). Second, and more importantly, they all say the same thing. They all say that Chooser should pick the shortest route in Salesman, and should Pass in Basketball. These theories all say that Chooser should pick the shortest route in Salesman in part because as stated they assume perfect mathematical competence. But they do not assume perfect foresight, so they don’t say Chooser should see into the future. It’s not just that they assume perfect mathematical competence, and that’s enough to solve Salesman. It’s that the particular skills that one needs to solve Salesman are also needed to apply any of the theories of decision on the market. Think about what it takes to solve Salesman by brute force. For each option, one has to look up 257 values, and sum them. Then one has to find the minimum value of the resulting sums. Think about what it takes to maximise expected utility by brute force.5 For each option, one has to calculate some values - products of utilities and probabilities - and sum the results of these calculations. Then one has to find the maximum value of the resulting sums. These are more or less the same skills. Anyone who can find the maximum expected utility of any of an arbitrarily large set of options can find the shortest path from an arbitrarily large set of possible paths. Applying the most popular decision theory we have, expected utility theory, requires exactly the same skill as solving problems like Salesman. And we said that the signature difference between Ideal and Non-Ideal Decision Theory was that the former did, but the latter did not, abstract away from the fact that ordinary humans cannot simply solve these problems. I’m not saying this to object to expected utility theory. Indeed, the theory I’m going to develop is just a version of expected utility theory. Rather, I’m saying it to support the claim that expected utility theory cannot be a theory of how to make good decisions. Instead, it’s a theory of what makes a decision good. A chooser shouldn’t calculate expected utilities for all their options. If they have, for example, $256! options, they probably won’t be able to. But a choice is good if it is the option with maximal expected utility. That’s to say, expected utility theory is a form of Ideal Decision Theory. And what goes for expected utility theory goes equally well for all theories out there. Treating any of them as a general theory of Non-Ideal Decision Theory would require that the person using them was able to solve problems at least as hard as the one in Salesman. And that’s not something we should assume in a theory that’s designed to be useful. Again, this isn’t an objection to these theories. It’s rather an argument that they should be interpreted as theories of Ideal Decision Theory. The only alternative is to interpret them in a way that Salesman is a quick refutation of each of them. That interpretation would be so uncharitable as to be wrong. Hence they are, as I said at the top of the subsection, attempts to offer an Ideal Decision Theory. 1.2.4 The Keynesian Critique I’m going to spend some time in chapter 3 defending the study of Ideal Decision Theory. But we shouldn’t give short shrift to the worry that maybe it shouldn’t be. There are plenty of modern arguments to this effect, but for now I just want to briefly mention of the classics. One of Keynes’s most famous, most notorious, quotes concerns the relationship between Ideal and Non-Ideal theory in realms including Decision Theory. It’s normally quoted wildly out of context, but here it is actually in context. But this long run is a misleading guide to current affairs. In the long run we are all dead. Economists set themselves too easy, too useless a task if in tempestuous seasons they can only tell us that when the storm is long past the ocean will be flat again. (Keynes 1923, 80, emphasis in original) Don’t focus on the temporal connotations of Keynes’s terminology of ‘long run’. What’s characteristic of his long run is not that it takes place in the distant future. What is characteristic of it instead is that it takes place in a world where some sources of interference are absent. It’s a world where we sail but there are no storms. It’s a study where we abstract away from storms and other unfortunate complications. And that’s what’s characteristic of Ideal Decision Theory. We know that people cannot easily solve problems like the one in Salesman. But we think that, at least some of the time, it’s worthwhile abstracting away from that particular human limitation. I think the connection between Ideal Decision Theory and the kind of equilibrium analysis that Keynes is critiquing here is even closer, since I think that notions of equilibria are at the heart of the right account of Ideal Decision Theory. So let’s say that Keynes’s target here is excessive abstraction, and that there are many views that might fall within that target. Classical economics is the proximal target, but Ideal Decision Theory may be another, various views in political philosophy might be yet another, and so on. And Keynes makes, as I read him, two distinct criticisms of these kinds of views: they are too easy, and they are useless. Both of these seem like they are good criticisms of the view that says of Salesman that Chooser should simply choose the shortest route. Saying that is indeed both easy and useless. In general though, the objection that these theories are ‘easy’ is not, I think, particularly fair. The most casual observation of twentieth century economics journals should make that clear. Even in the case Keynes uses, it isn’t quite obvious why the sea should be flat once the storm passes. It won’t be perfectly flat; the tides are not the result of any storm. But couldn’t a big enough storm make semi-permanent shifts to the state of the ocean? If not, it takes some work to show this. A bit more seriously, by the time of the General Theory, Keynes himself thought economic storms could lead to equilibria with bad characteristics (Keynes 1936). In economics, some storms do not leave flat seas in their wake. On the other hand, the objection that we set ourselves too “useless” a task if we confine ourselves to Ideal Theory does ring true. Note, however, an important detail to this complaint. It’s not that Ideal Theory is useless. It’s that if we do nothing but Ideal Theory, if we only say that when the storm is over the seas are flat, we are not sufficiently useful. That all seems right, but it doesn’t show that we should never do Ideal Theory. It just shows that we should never settle for just doing it. Ideal Theory is an abstraction. In decision theory, it abstracts away from mathematical limitations. In economics, it might abstract away from various idiosyncratic features of individual economies. There is nothing wrong with abstraction as such. Without any abstraction, we just have a buzzing, blooming confusion of data. That’s to say, for any problem, there is some abstraction that will be helpful in solving it. But there’s a quantifier shift fallacy that’s dangerously tempting around here. We might infer from every problem needs some abstraction, that there is some abstraction that is needed for every problem. And that latter claim is false. It’s when you believe something like that that you end up settling for Ideal Theory, and set yourself, if not always too easy, then definitely too useless a task. We should do more than Ideal Theory. But the rule there is collective. We, collectively, should do more than Ideal Theory. It doesn’t follow that any one person, or any one book, must do more. And this book isn’t, for the most part, going to do any more; this is a work in Ideal Theory. But I’ve belabored this point about the role of Ideal Theory in inquiry because the approach this book takes to Ideal Theory doesn’t make sense if you see Ideal Theory as the pinnacle, or as the center, of inquiry. It’s one task among many, and it has its limits. And the approach I’m going to take will make those limits clearer, and only make sense in light of those limits. 1.3 Proceduralism and Ratificationism The vast bulk of decision theories on the market are what I’ll call proceduralist. The theory I’m going to offer, causal ratificationism, is not. Many people, I suspect, will see that as reason enough to reject causal ratificationism. But this would be a mistake. As I’ll argue in this section, we should want our Non-Ideal Decision Theory to be proceduralist. But there is no obvious reason why an Ideal Decision Theory should be proceduralist. 1.3.1 Proceduralism Defined A proceduralist decision theory provides a procedure for determining what to do in a given problem. The procedure might not be particularly efficient, or even remotely practical. I’ll count things like Add up the lengths in each path, then do a selection sort on the results, as a procedure for solving Salesman, even if it would never complete in the available time. But let’s abstract away from those details for now. Instead of focusing on computational complexity, I’ll understand proceduralism about decision problems as the conjunction of four claims. Two of these are about the inputs to deliberation, and two of them are about the outputs. Here are the four claims, with some hopefully useful names. Ex Ante What is to be done is a function of what things are like at the start of deliberation. Transparency The inputs to that function are all knowable as long as the chooser is sufficiently rational and self-aware. Decisiveness In any decision problem, there is only one thing which is to be done, unless there are several things that are equally good. In the latter case, adding a minimal sweetening to any one of the equally good options would make it the option to be chosen. Possibility In any finite decision problem, at least one choice is rationally permissible. That is, there are no finite rational dilemmas. I’m going to argue against all four of these claims. The kind of theory I favor, ratificationism, instead endorses the following four claims. Ex Post What is to be done is a function of what things are like at the end of deliberation. Opacity The inputs to that function involve, among other things, what probabilities are rational given the chooser’s evidence, and this may be opaque to the self-aware, rational agent. Indecisiveness In many decision problems, there are permissible options which are not equally good, and there would still be many permissible options after sweetening one or other option. Dilemmas In many finite decision problems, no choice is rationally permissible. That is, there are finite rational dilemmas. Neither proceduralism nor ratificationism is a package deal; you can mix and match the parts. And there are many natural weakenings of one or other part of the family of views. For instance, in chapter 4, I’ll spend some time on views that say that Decisiveness is only guaranteed to hold in cases where there are just two options. But that said, there are natural affinities between the four parts of proceduralism. if you thought the point of decision theory was to provide a user’s guide to making decisions, you’ll naturally end up with a proceduralist theory. And lots of theories have done just that. Any theory which starts with a function from states available to the chooser at the start of deliberation to numerical values, and instructs the chooser to maximise that value, will be proceduralist. That very abstract description of a decision theory covers the vast majority of theories on the market today. Any theory of decision that assigns numerical values to each option on the basis of factors accessible to the chooser at the start of deliberation, and then exhorts the chooser to choose the option with the highest value, will be proceduralist. And if you’re familiar with contemporary work in decision theory, you’ll know that most theories on the market do indeed assign numerical values to each option on the basis of factors accessible to the chooser at the start of deliberation, and then exhort the chooser to choose the option with the highest value. In a recent paper, Adam Elga (n.d.) describes a class of theories he calls ‘suppositional’, and notes that most existing theories of decision are suppositional. (And goes on to argue that we should want a theory to be suppositional.) The suppositional theories, as he describes them, are a subset of the procedural theories. Indeed, they are a subset of the theories described in the last paragraph - those that assign numerical values to choices. So saying that most theories on the market are proceduralist is not saying anything new. If anything, it’s a commonplace. NOT COMPLETE 1.4 Causal Ratificationism It’s going to take some setup to articulate precisely the positive theory I’m going to defend in this book. But I think it’s worth having a rough statement of it up front, so you can see where we’re headed. According to causal ratificationism, a choice is rational if the following two conditions are met. First, at the time the choice is made, no other choice has higher expected utility, given some probability distribution over the states that is rational at that time. Second, at the time the choice is made, no other choice weakly dominates the choice. There are a lot of technical terms there which I’ll make clearer as this chapter goes along. But the key thing NOT COMPLETE The dubbing is in Robinson (1949). For a thorough history of the problem, see Schrijver (2005). For an accessible history of the problem, which includes these references, see the wikipedia page on ‘Traveling Salesman Problem’.↩︎ The 256 cities are the cities in the lower 48 states from the 312 cities in North America that John Burkardt mapped in his dataset Cities, available at people.sc.fsu.edu/~jburkardt/datasets/cities/cities.html.↩︎ I’m borrowing the term ‘non-ideal’ from work in political philosophy. See Valentini (2012) for a good survey of some relevant work, and Mills (2005) for an important critique of the centrality of ideal theory in political philosophy. Critics of ideal theory, such as Mills, and Sen (2006), argue that we shouldn’t base non-ideal theory on ideal theory. I’m going to agree, but my focus is primarily in the other direction. I’m going to argue that it isn’t a constraint on ideal theory that it is useful in constructing a non-ideal theory.↩︎ The full list of cities in the Salesman problem is: Abilene, TX; Akron, OH; Albany, NY; Albuquerque, NM; Allentown, PA; Amarillo, TX; Ann Arbor, MI; Asheville, NC; Ashland, KY; Atlanta, GA; Atlantic City, NJ; Augusta, GA; Augusta, ME; Austin, TX; Bakersfield, CA; Baltimore, MD; Bangor, ME; Baton Rouge, LA; Battle Creek, MI; Bay City, MI; Beaumont, TX; Bellingham, WA; Berkeley, CA; Billings, MT; Biloxi, MS; Binghamtom, NY; Birmingham, AL; Bismarck, ND; Bloomington, IL; Boise, ID; Boston, MA; Bowling Green, KY; Brattleboro, VT; Bridgeport, CT; Brockton, MA; Buffalo, NY; Burlington, VT; Butte, MT; Cambridge, MA; Canton, OH; Carson City, NV; Cedar Rapids, IA; Central Islip, NY; Champaign, IL; Charleston, SC; Charleston, WV; Charlotte, NC; Chattanooga, TN; Cheyenne, WY; Chicago, IL; Cincinnati, OH; Cleveland, OH; Colorado Springs, CO; Columbia, MO; Columbia, SC; Columbus, GA; Columbus, OH; Concord, NH; Corpus Christi, TX; Dallas, TX; Dayton, OH; Daytona Beach, FL; Decatur, IL; Denver, CO; Des Moines, IA; Detroit, MI; Dodge City, KS; Dubuque, IA; Duluth, MN; Durham, NC; Eau Claire, WI; Elizabeth, NJ; El Paso, TX; Enid, OK; Erie, PA; Eugene, OR; Eureka, CA; Evansville, IN; Fall River, MA; Fargo, ND; Flagstaff, AZ; Flint, MI; Ft Smith, AR; Ft Wayne, IN; Ft Worth, TX; Fresno, CA; Gadsden, AL; Gainesville, FL; Gallup, NM; Galveston, TX; Gary, IN; Grand Junction, CO; Grand Rapids, MI; Great Falls, MT; Green Bay, WI; Greensboro, NC; Greenville, SC; Gulfport, MS; Hamilton, OH; Harrisburg, PA; Hartford, CT; Helena, MT; Houston, TX; Huntsville, AL; Indianapolis, IN; Iowa City, IA; Jackson, MI; Jackson, MS; Jacksonville, FL; Jersey City, NJ; Johnstown, PA; Joplin, MO; Kalamazoo, MI; Kansas City, KS; Kansas City, MO; Kenosha, WI; Key West, FL; Knoxville, TN; Lafayette, IN; Lancaster, PA; Lansing, MI; Laredo, TX; Las Vegas, NV; Lawrence, MA; Lexington, KY; Lima, OH; Lincoln, NE; Little Rock, AR; Los Angeles, CA; Louisville, KY; Lowell, MA; Lubbock, TX; Macon, GA; Madison, WI; Manchester, NH; Marshall, TX; Memphis, TN; Meriden, CT; Miami, FL; Milwaukee, WI; Minneapolis, MN; Minot, ND; Mobile, AL; Montgomery, AL; Montpelier, VT; Muncie, IN; Nashville, TN; Natchez, MS; Newark, NJ; New Britain,CT; New Haven, CT; New Orleans, LA; New York, NY; Norfolk, VA; Oakland, CA; Ogden, UT; Oklahoma City, OK; Omaha, NE; Orlando, FL; Paducah, KY; Pasadena, CA; Paterson, NJ; Pensacola, FL; Peoria, IL; Philadelphia, PA; Phoenix, AZ; Pierre, SD; Pittsburgh, PA; Pittsfield, MA; Pocatello, ID; Port Arthur, TX; Portland, ME; Portland, OR; Portsmouth, NH; Portsmouth, VA; Providence, RI; Provo, UT; Pueblo, CO; Racine, WI; Raleigh, NC; Rapid City, SD; Reading, PA; Reno, NV; Richmond, VA; Roanoke, VA; Rochester, MN; Rochester, NY; Rockford, IL; Sacramento, CA; Saginaw, MI; Saint Cloud, MN; Saint Joseph, MO; Saint Louis, MO; Saint Paul, MN; Saint Petersburg, FL; Salem, OR; Salina, KS; Salt Lake City, UT; San Antonio, TX; San Bernardino, CA; San Diego, CA; San Francisco, CA; San Jose, CA; Santa Barbara, CA; Santa Cruz, CA; Santa Fe, NM; Sarasota, FL; Savannah, GA; Schenectady, NY; Seattle, WA; Sheboygan, WI; Sheridan, WY; Shreveport, LA; Sioux City, IA; Sioux Falls, SD; South Bend, IN; Spartanburg, NC; Spokane, WA; Springfield, IL; Springfield, MA; Springfield, MO; Springfield, OH; Stamford, CT; Steubenville, OH; Stockton, CA; Superior, WI; Syracuse, NY; Tacoma, WA; Tallahassee, FL; Tampa, FL; Terre Haute, IN; Texarkana, TX; Toledo, OH; Topeka, KS; Trenton, NJ; Troy, NY; Tucson, AZ; Tulsa, OK; Urbana, IL; Utica, NY; Waco, TX; Walla Walla, WA; Washington, DC; Waterloo, IA; West Palm Beach, FL; Wheeling, WV; White Plains, NY; Wichita, KS; Wilkes-Barre, PA; Wilmington, DE; Wilmington, NC; Winston-Salem, NC; Worcester, MA; Yakima, WA; Youngstown, OH; Yuma, AZ; Zanesville, OH.↩︎ I’ll say more about what that means in a bit. For now, see Briggs (2019) for the details on expected utility.↩︎ "],["introchap.html", "Chapter 2 Make Ratifiable Decisions 2.1 Basic Decision Theory 2.2 Some Theories of Decision 2.3 Games and Decisions 2.4 Plan for the Book", " Chapter 2 Make Ratifiable Decisions A rational chooser knows what they are doing, and thinks that it is for the best. That is, they think that there is nothing else they could be doing that would be better. This book defends a version of decision theory that starts, and largely ends, with this principle. Properly understood, this is all there is to decision theory. But how to properly understand it will be the subject of much of this book. The principle I just stated is backwards looking. It says that the chooser must think that the decision is for the best when they make it. It doesn’t say much about how they come to make that decision, or whether the decision makes sense given the views the chooser has at the start of deliberation. That’s by design. Decision theory is the theory of when decisions can be defended. Or, at least, that’s what I’ll argue in this book. I’m mostly going to be concerned with a special class of decision problems: those involving demons who have spectacular predictive powers. These have been a particular focus of decision theorists for the last half-century. In keeping them center stage I am, in this one respect at least, following tradition. But I will make use of the principle that our theory of how to make decisions when demons are around should be consistent with our theory of how to make decisions when demons are not around. And the motivations for the two parts of the theory should be consistent as well. This turns out to be a somewhat substantive constraint. Demons predict what other people will choose, make moves accordingly, and these moves make a difference to the consequences of other choices. That’s to say, demons behave just like the rational players in orthodox game theory. Interacting with demons is, at a fairly deep level, playing games with them. So we should expect game theory to have something to tell us about how those interactions go. This isn’t a novel point; I owe it to William Harper (1986). But it is going to be central to the plot of this book. The next three sections spell out the points made in each of the last three paragraphs. And then I’ll close the chapter by setting out a generic version of the main example of the book, and going over the plans for the rest of the book. 2.1 Basic Decision Theory 2.1.1 Simple Decision Problems A simple decision problem starts with a table like 2.1. Table 2.1: A generic two-by-two decision table. L R U \\(v_{11}\\) \\(v_{12}\\) D \\(v_{21}\\) \\(v_{22}\\) The rows we the options that the chooser, who I’ll mostly call Chooser from now on, has. The columns list the possible states of the world. And in the cells set out the value to Chooser of each of these option-state pairs. Just to make the notation easier to remember, I’ve written \\(v_{ij}\\) for the value of the outcome when Chooser selects option in row \\(i\\), and the world is in state \\(j\\). Now there are a lot of questions one could have about that last paragraph, and I’ll spend section D going over five such questions at some length. But for now let’s start with that basic picture. In general there are more than two possible options and more than two possible states, but let’s start with this simple case for now and work up to the more complicated cases. In order to make a decision, Chooser typically needs one more piece of information - how likely are the states \\(L\\) and \\(R\\)? Let’s add that information in: the probability of \\(L\\) is \\(p_1\\) and the probability of \\(R\\) is \\(p_2\\). What do we mean by ‘probability’ here? Good question, and one that I’ll spend a lot of time on in chapter 6. For now, just use its colloquial meaning. 2.1.2 Defining Basic Decision Theory Given all these facts, Chooser can assign a value to an option using the following formulae: \\[\\begin{align*} V(U) &amp;= p_1 v_{11} + p_2v_{12} \\\\ V(D) &amp;= p_1 v_{21} + p_2v_{22} \\end{align*}\\] And Chooser is rational iff they choose an option with maximal value. In the more general case where \\(O_i\\) is an arbitrary option, there are \\(k\\) possible states, the probability of state \\(S_k\\) is \\(p_k\\), and the value of the combination of option \\(O_i\\) and state is \\(S_k\\), the value of option \\(O_i\\) is given by this formula. \\[ V(O_i) = \\sum_{j=1}^k p_j v_{ij} \\] And in the even more general case where there are continuum many states, we need to use integrals to work out the value of each option. But these complexities aren’t going to be particularly relevant to our story, so I’ll return to the special two choice two option case, and note when the extra generalities are needed. The formulae above are what are usually called the expected values of each option, and the decision theory I’ve just state is that if Chooser is rational, they maximise the expected value of their choice, given this probability distribution over the states of the world. Let’s restate that not using variables, but explicitly using probabilities and values. Here \\(Pr\\) is the probability function relevant to Chooser’s decision, and \\(V\\) is the value function. I’ll use concatenation for conjunction, so \\(UL\\) means that both \\(U\\) and \\(L\\) are true, i.e., that the first option is chosen and the first state is actualised. And let \\(O\\) be an option, in this case a member of {U, D}. \\[ V(O) = Pr(L) V(OL) + Pr(R) V(OR) \\] I’ll call the theory that values each option this way, and says that rational choosers maximise value, Basic Decision Theory. It could just as easily be called the Crude Savage Decision Theory. The ‘Savage’ there is because the formula at the heart of it is the same formula that Savage (1954) puts at the heart of his decision theory. But the ‘Crude’ is there because Basic Decision Theory leaves off all that Savage says about the nature of options and states. Steele and Stefánsson (2020, sect 3.1) have a good summary of what Savage says here. I’m not going to go into that. Instead I’ll note why something more needs to be said. As it stands, Basic Decision Theory leads to absurd outcomes. 2.1.3 Why Basic Decision Theory Fails Consider the St. Crispin’s Day speech that Shakespeare has Henry V give before the Battle of Agincourt. (I’m indebted to a long ago conversation with Jamie Dreier for pointing out the connection between this speech and decision theory.) The background is that the English are about to go to battle with the French, and they are heavily outnumbered. Westmoreland wants to wait for more troops, and Henry does not, offering this reasoning. What’s he that wishes so? My cousin Westmoreland? No, my fair cousin; If we are marked to die, we are enough To do our country loss; and if to live, The fewer men, the greater share of honor. God’s will! I pray thee, wish not one man more. It looks like Henry is suggesting that table 2.2 is the right model for their decision. Table 2.2: Henry’s reasoning on St. Crispin’s Day. Victory Defeat Attack \\(a\\) \\(c\\) Wait \\(b\\) \\(d\\) Henry argues, not implausibly, that \\(a &gt; b\\) since they will get more honour, and \\(c &gt; d\\), since they will lose fewer men. Now it doesn’t matter what the probabilities of victory and defeat are. To see this, call them \\(x\\) and \\(y\\) respectively, and note that the two facts Henry mentions suffice to guarantee that \\(ax + cy &gt; bx + dy\\), assuming just that the probabilities are non-negative. So great, Henry is right, and they should attack. And they do, and they win, and all’s well that end’s well. No! That’s an absurd decision, even if it ended well on this occasion. Note that Henry’s reasoning here is completely general - you could say the same before every battle. And it isn’t true that you should always rush into battle with whoever you have on hand. At one level, it’s easy to say what has gone wrong here. There is too tight a connection between the choice Henry makes, and which state of the world is actual. But, and this is the philosophical problem, just what is the problematic connection between the choice and the state? This isn’t obvious, because there are two different connections between the choice and the state, and it’s a matter of some philosophical import which of them matters. On the one hand, there is an evidential connection between the choice and the state. Learning that Henry has decided to go to battle rather than wait for reinforcements is evidence that he will lose. Misleading evidence, as it turned out, but certainly evidence. And maybe what’s gone wrong in Henry’s reasoning is that he has ignored that evidential connection in his reasoning. On the other hand, there is a causal connection between the choice and the state. It’s possible, given Henry’s evidential situation at the time he makes the decision, not just that he is defeated, but that his rushing into battle causes his defeat. Relatedly, it’s possible, for all Henry knows, that rushing into battle lowered the objective chance of his winning. Those last two sentences didn’t quite say the same thing, and the differences between them will matter a little going forward, but for now we’ll slide over their differences, and just note that there is in some natural sense a causal connection between Henry’s decision and the resulting state of the battle. So here we get to a point of common ground among contemporary decision theorists. It will, more or less, be the last point of common ground on the journey; from here everything gets contentious. When there is both an evidential and a causal connection between the possible choices and the possible states of the world, it is inappropriate to use Basic Decision Theory to make a decision. Indeed, in these cases, Basic Decision Theory will often validate the wrong decision. It’s not quite a universal view, and I’ll talk a bit in appendix E about people who don’t believe it, but there is another very widely accepted claim in the vicinity of this one. When there is no evidential or causal connection between the possible choices and the possible states, most theorists think Basic Decision Theory recommends the right choice. Now they might not say, and typically do not say, that it makes the right recommendations for the right reasons. But they do say, at least most of them, that it gets the verdicts right. In the favored lingo of twentieth century philosophers, it is extensionally adequate in these cases. So that covers the cases where there is both an evidential and causal connection - Basic Decision Theory gets things wrong - and the cases where there is neither - Basic Decision Theory gets things right. But what about the cases where there is one such connection but not the other? We’re all taught that correlation is not causation. What happens when there is correlation but not causation between the choices and the states? Then things get really interesting, and that’s the debate we’re doing to jump into. 2.2 Some Theories of Decision 2.2.1 Newcomb Problems It’s not at all obvious how there could be a case where the possible choices and possible states could be causally connected but not evidentially connected. I’m going to set the possibility of such a case aside, at least until someone shows me what such a case might look like. Because there is a very natural way that the choices and states could be evidentially but not causally connected: they could have a common cause. And one way that could come about is if the states are predictions of Chooser’s choice, made by someone who has a deep insight into Chooser’s choice dispositions. We’ll call that someone Demon, and a decision problem in which the states are based in Demon’s predictions a Demonic decision problem. I’ll have much more to say about demons in section B, but for now all we need to know is that Demon has the means, motive, and opportunity to correctly predict what strategy a Chooser will adopt. The most famous Demonic decision problem is Newcomb’s Problem (Nozick 1969). Chooser is presented with two boxes, one opaque and one clear. They have a choice between taking just the opaque box, i.e., taking one box, and taking both the opaque and the clear box, i.e., taking two boxes. The clear box has a good of value \\(y\\) in it. The contents of the opaque box are unknown. Demon has predicted the chooser’s choice, and has placed a good of value \\(x\\) in it if they predict Chooser will take one box, and left it empty (which we’ll assume has value 0) if they predict Chooser will take both boxes. The key constraint is \\(x &gt; y\\). In most versions the value given for \\(x\\) is massively greater than that for \\(y\\), but the theories that are developed for the problem typically are sensitive only to whether \\(x\\) is larger than \\(y\\), not to how much larger it is. Demon is really good at their job. They are not a time traveller; they are making a prediction that is not causally influenced by what the Chooser actually does. But they are really good. I’ll assume that they are arbitrarily good, and come back to just what I mean by that in section B. I’ll write 1 and 2 for the two choices, and P1 and P2 for the predictions. In general, where there is a demon who makes these kinds of predictions, I’ll write ‘PX’ to mean the state of choice X being predicted. Table 2.3 sets out the problem. Table 2.3: Newcomb’s Problem P1 P2 1 \\(x\\) 0 2 \\(x + y\\) \\(y\\) A large part of late 20th Century decision theory was given over to discussing this problem. So-called Causal Decision Theorists argued in favor of taking both boxes. The primary argument is that whatever the demon has done, the chooser gets a bonus of \\(y\\) for taking the second box. It’s good to get guaranteed bonuses, so they should take the bonus. This is basically the view I’m going to defend in this book, though with a number of deviations from the way it was defended in these classic works. So-called Evidential Decision Theorists argued in favor of taking just the one box. The primary argument is that the chooser who takes one box expects to get \\(x\\), the chooser who expects to get both boxes expects to get \\(y\\), it’s better to take a choice that one expects to do better, and \\(x &gt; y\\), so it’s better to take one box. Both of these arguments trace back to the original presentation of the problem by Nozick. He named the problem after William Newcomb, a physicist from whom he learned of the problem. For much more detail on the background to this problem, and the motivation for the two prominent solutions, see Weirich (2020). Let’s turn to looking at those two solutions in more detail. 2.2.2 Introducing EDT and CDT The two most famous theories in recent work in decision theory are Causal Decision Theory and Evidential Decision Theory. I used these terms in subsection 2.2.1 without defining them. It’s time to do that now. As I understand the way the terms are used, and indeed as I’ll be using them, they are potentially misleading. Both of these are not really theories, but families of theories. Evidential Decision Theory (EDT) is a somewhat tighter family of theories than Causal Decision Theory (CDT), but neither is something that I would typically be happy calling a theory. In this section I’ll give somewhat imprecise descriptions of each ‘theory’, starting with EDT. In section 2.2.3 I’ll say why both of these are really theory schema, and set out some of the more viable ways of making them into precise theories. EDT, as I’m going to understand it, traces back to the first edition of Richard Jeffrey’s The Logic of Decision (Jeffrey 1965). The idea behind it is that what goes wrong with Henry’s reasoning at Agincourt is that he ignores the fact that rushing into battle lowers the probability that he will win. In fact, according to EDT, the probability that he will win doesn’t really matter to his decision. What matters is the probability that he will win if he attacks, and the probability that he will win if he waits for reinforcements. The value of each choice, according to EDT, is given by this formula \\[ V(O_i) = Pr(S_1 | O_i) V(S_1 O_i) + Pr(S_2 | O_i) V(S_2 O_i) \\] And, as before, the one rule in decision theory is that one should maximise value. So EDT says that Basic Decision Theory is incorrect. It uses probabilities of states, i.e., terms like \\(Pr(S_j)\\), where it should use probabilities of states given choices, i.e., terms like \\(Pr(S_j | O_i)\\). That’s what goes wrong with Henry’s reasoning. In Newcomb’s Problem, EDT says that one should take one box. Assume, for simplicity, that the probability that the demon will make correct predictions is 1. Then the value of taking one box is \\(x\\), the value of taking two boxes is \\(y\\), and by hypothesis \\(x &gt;&gt;&gt; y\\), so one should take one box. CDT, or at least the version we’re going to focus on for a while, traces back to David Lewis’s paper Causal Decision Theory (Lewis 1981a). Lewis actually has two aims in this paper: to set out a version of CDT, and to argue that the other versions don’t differ in significant ways from his version. It’s going to be somewhat important to the plotline of this book that Lewis’s second claim, that the various versions of CDT don’t greatly differ, is false. But the positive theory Lewis presents is interesting whether or not that second claim goes through, and that’s what we’ll focus on. The idea is that Basic Decision Theory was not incorrect, as EDT says, but incomplete. It needs to be supplemented with rules about when the formula can be applied. In particular, we need to add that the states have to be causally independent of the options. In Lewis’s terminology, the states have to be ‘dependency hypotheses’. Each dependency hypothesis is something that the chooser has no causal influence over, and which determines, in conjunction with each possible act by the chooser, the probability of each possible outcome. If you apply the formula from Basic Decision Theory to cases where the states themselves depend (or even may depend) on the option, things go wrong. That’s what CDT says goes wrong in Henry’s case. It’s the right formula, and he applies it correctly, but he shouldn’t have started with simply win and lose as the states. Rather, he should have started with dependency hypotheses that do not causally depend upon his choices. For example, he could have started with the following three hypotheses: the troops we have now are enough to win; the troops we have now are not enough to win, but the troops we will have after reinforcements will be enough to win; and, even after getting reinforcements, we won’t have enough troops to win. Since the middle of those states is very likely, and the utility of waiting for reinforcements is higher in that state, he probably should have waited. In Newcomb’s Problem, CDT says that one should take both boxes. What Demon predicts is not causally dependent on what Chooser selects. So we can use P1 and P2 as states. Let \\(z\\) be the probability of P1, and hence the probability of P2 is \\(1-z\\). Then the expected value of taking one box is \\(zx\\), while the expected value of taking two boxes is \\(zx + y\\). Without yet knowing what \\(z\\) is, a question that will become rather important as we go on, we know that \\(zx + y &gt; zx\\), so taking two boxes has higher value. So that’s what one should do. 2.2.3 Making The Theories Precise So that’s the basic picture of EDT and CDT. But as I alluded to earlier, setting out the basic picture isn’t quite the same thing as setting out a theory. In this section I’ll flag some factors that need to be settled to turn them into a theory. What are probabilities? Are they ex ante or ex post? NOT COMPLETE 2.2.4 Non-Unique Solutions Especially if using ex post Say about 1; almost dominance Say about 2; basically link to decisiveness chapter Say about 0; basically link to dilemmas chapter NOT COMPLETE 2.2.5 Ratificationism Just give a definition NOT COMPLETE 2.3 Games and Decisions This section goes into a bit of detail about the connection between game theory and decision theory. If you want much more background on game theory, I’ve included some explanations of the key concepts in Appendix A. The point of this section is that the connection between game theory and decision theory is much tighter than a lot of theorists have realised. 2.3.1 Newcomb Games Let’s start with an interesting variant of Newcomb’s Puzzle, one which is used to good effect by Frank Arntzenius (2008). Keep the contents of the boxes the same, including that Demon puts \\(x\\) into the first box iff they predict only one box will be taken. But this time both boxes are clear. Now Chooser can see exactly what is in each box before making a decision. What should they do? We can model this problem using the tree in figure 2.1. Figure 2.1: Newcomb’s Problem with both boxes transparent This representation should look familiar from game theory textbooks. It’s just a standard extensive form representation of a game where each player makes one move. Since we’ll be using trees like this a bit, I want to explain the notation. The game starts at the hollow node, which in this case is at the top of the tree. At each node, we move along a path from wherever we are to a subsequent node. So each node gets labeled with who is making the choice, and the edges get labeled with the choices they can make. This game starts with the Demon predicting either that Chooser takes 1 box - this is the edge labeled P1 - or that Chooser takes 2 boxes. Either way we get to a node where Chooser moves, either by taking 1 box or 2. It’s a solid node, which means (in the notation of this book) that it’s not where the game starts, and it’s not where the game ends. Then whatever happens, we get to a terminal node, here denoted with a square. At each terminal node we list the payouts. But here we only listed the payouts to Chooser. To make something really into a game, there should be payouts for both players. What are Demon’s payouts? Well, what makes something the payout function for a player is that it takes higher values the more they get what they want. Since Demon is trying to predict player, they want situations where they predict them well. So we can simply say that their payout is 1 for a correct prediction, and 0 for an incorrect prediction. That suggests the tree for Arntzenius’s ‘transparent box’ version of Newcomb’s Problem should look like figure 2.2. Figure 2.2: Newcomb’s Problem with both boxes transparent, and Demon’s payouts listed I’ve put Demon’s payouts second, even though Demon moves first. The focus here is on Chooser, so they are player 1. When a game representation lists the payout in a situation as \\(a, b\\) that means that player 1 gets \\(a\\) and player 2 gets \\(b\\). In this case that means the chooser gets \\(a\\) and the demon gets \\(b\\). In this book I’m mostly going to work with games where Demon’s payouts are either 1 for a correct prediction of 0 for an incorrect one. But once we’ve got the basic concept of Demon as a player getting payouts, we can set the demon up with other payouts too. And then we can bring just about any tool we like from contemporary game theory to bear on demonic decision theory. That move, of treating Newcomb Problems as games, is taken straight from work by William Harper (1986). And it is going to be the central move in this book. When we follow Harper’s lead and transform the original Newcomb Problem into a game, we get table 2.4. Table 2.4: Newcomb’s Problem as a Game P1 P2 1 \\(x, 1\\) \\(0, 0\\) 2 \\(x + y, 0\\) \\(y, 1\\) Or, at least, that’s the so-called strategic form of the game. We can also represent it, as in figure 2.3 as a game that takes place over time. Figure 2.3: Newcomb’s Problem with first box hidden, and Demon’s payouts listed The dashed line there represents that those two nodes are in what game theorists call an information set. That means that when the player to move reaches one of those nodes, all they know is that they are at one of these nodes and not any other. In this case, Chooser knows that they have to select 1 box or 2, and they know the payouts given their choice and Demon’s prediction. But they do not know what Demon predicted, so they do not know which node they are at. This extensive form representation is in a way more accurate than the strategic form representation in the table above. It encodes that Demon goes first, which is something usually stressed in the story that is told about Newcomb’s Problem. But the table form is easier to read, and makes clearer that there is only one equilibrium of the game: Demon makes prediction P2 and Chooser chooses 2. So I’ll mostly use tables when they are possible. And they often are possible - lots of games can be turned into demonic decision problems like Newcomb’s Problem. 2.3.2 Familiar Games Much of what happens in this book comes from seeing demonic decision problems as games and, conversely, seeing games as potential demonic decision problems. So I want to spend a little time setting out how the translation between the two works. Transforming a demonic decision problem into a game is easy. As I noted, you just replace the states generated by Demon’s choices with moves for Demon, and give them payout 1 if they predict correctly, and 0 otherwise. You might worry that this only gives you cases where Demon is approximately perfect, but we also want cases where the demon is, say, 80% accurate. But that’s easy to do as well. In fact there are two ways to do it. The first is what I’ll call the Selten strategy, because it gives the demon a ‘trembling hand’ in the sense of Selten (1975). Instead of letting Demon choose a state in the original problem, let Demon choose one of \\(n\\) buttons, where \\(n\\) is the number of choices the (human) chooser has. Each button is connected to a probabilistic device that generates one of the original states. If you want Demon to be 80% accurate, say the button \\(b_i\\) associated with option \\(o_i\\) outputs state \\(s_i\\) with probability 0.8, and each of the other states with probability \\(\\frac{0.2}{n - 1}\\). And still say that Demon gets payout 1 for any \\(i\\) if the chooser selects \\(o_i\\) and the button generates state \\(s_i\\), and 0 otherwise. The second is what I’ll call the Smullyan strategy, because it involves a Knights and Knaves puzzle of the kind that play a role in several of Smullyan’s books, especially his (1978). Here the randomisation takes place before Demon’s choice. Demon is assigned a type Knight or Knave. Demon is told of the assignment, but Chooser is not. If Demon is assigned type Knight, the payouts stay the same as in the game where Demon is arbitrarily accurate. If Demon is assigned type Knave, the payouts are reversed, and Demon gets payout 1 for an incorrect prediction. There are benefits to each approach, and there are slightly different edge cases that are handled better by one or other version. I’m mostly going to stick to cases where Demon is arbitrarily accurate, but I need these on the table to talk about cases others raise where Demon is only 75-80% accurate. And in general either will work for turning a demonic decision problem into a game. Turning games into demonic decision problems is a bit more interesting. Start with a completely generic two-player, two-option, simultaneous move, symmetric game, as shown in table 2.5. We won’t only look at symmetric games, but it’s a nice way to start. Table 2.5: A generic symmetric game. A B A \\(x, x\\) \\(y, z\\) B \\(z, y\\) \\(w, w\\) In words, what this says is that each player chooses either A or B. If they both choose A, they both get \\(x\\). If they both choose B, they both get \\(w\\). And if one chooses A and the other chooses B, the one who chooses A gets \\(y\\) and the one who chooses B gets \\(z\\). (Note that the payouts list row’s payment first, if you’re struggling to translate between the table and the text.) A lot of famous games can be defined in terms of restrictions on the four payout values. For example, a game like this is a Prisoners’ Dilemma if the following constraints are met. \\(x &gt; z\\) \\(y &gt; w\\) \\(w &gt; x\\) Some books will also add \\(2x &gt; y + z\\) as a further constraint, but I’ll stick with these three. Now to turn a game into a demonic decision problem, first replace column’s payouts with 1s and 0s, with 1s along the main diagonal, and 0s everywhere else. Table 2.6 shows what a generic symmetric game looks like after this transformation. Table 2.6: The demonic version of a generic symmetric game. A B A \\(x, 1\\) \\(y, 0\\) B \\(z, 0\\) \\(w, 1\\) The next step is to replace Demon’s moves with states that are generated by Demon’s predictions. As before, I’ll put ‘P’ in front of a choice name to indicate the state of that choice being predicted. The result is table 2.7. Table 2.7: The demonic decision problem generated by a generic symmetric game. PA PB A \\(x\\) \\(y\\) B \\(z\\) \\(w\\) If we add the constraints \\(x &gt; z, y &gt; w, w &gt; x\\), this is essentially a Newcomb Problem. I’m a long way from the first to point out the connections between Prisoners’ Dilemma and Newcomb’s Problem; it’s literally in the title of a David Lewis paper (Lewis 1979). But what I want to stress here is the recipe for turning a familiar game into a demonic problem. We can do the same thing with Chicken. The toy story behind Chicken is that two cars are facing off at the end of a road. They will drive straight at each other, and at the last second, each driver will choose to swerve off the road, which we’ll call option A, or stay on the road, which we’ll call option B. If one swerves and the other stays, the one who stays is the winner. If they both swerve they both lose and it’s boring, and if they both stay it’s a fiery crash. So in terms of the payouts in the general symmetric game, the constraints are: \\(x &lt; z\\) \\(y &gt;&gt; w\\) \\(x &gt;&gt; w\\) Just what it means for one value to be much more than another, which is what I mean by ‘\\(&gt;&gt;\\)’, is obviously vague. Table 2.8 gives an example with some numbers that should satisfy it. Table 2.8: A version of Chicken. A B A \\(0, 0\\) \\(0, 1\\) B \\(1, 0\\) \\(-100, -100\\) Replace the other driver, the one who plays column in this version, with a Demon, who only wants to predict row’s move. The result is table 2.9. Table 2.9: A demonic version of Chicken. A B A \\(0, 1\\) \\(0, 0\\) B \\(1, 0\\) \\(-100, 1\\) All I’ve done to generate table 2.9 is replace column’s payouts with 1s on the main diagonal, and 0s elsewhere. The next step is to replace the demonic player with states generated by Demon’s choices. The result is table 2.10. Table 2.10: A demonic decision problem based on Chicken. pa pb a \\(0\\) \\(0\\) b \\(1\\) \\(-100\\) And table 2.10 is just the Psychopath Button example that Andy Egan (2007) raises as a problem for Causal Decision Theory. Another familiar game from introductory game theory textbooks is matching pennies. This is a somewhat simplified version of rock-paper-scissors. Each player has a penny, and they reveal their penny simultaneously. They can either show it with the heads side up (option A), or the tails side up (option B). We specify in advance who wins if they show the same way, and who wins if they show opposite ways. So let’s say column will win if both coins are heads or both are tails, and row will win if they are different. The payouts are shown in table 2.11. Table 2.11: The game matching pennies. A B A \\(0, 1\\) \\(1, 0\\) B \\(1, 0\\) \\(0, 1\\) This isn’t a symmetric game, but it is already demonic. Column’s payouts are 1 in the main diagonal and 0 elsewhere. So we can convert it to a demonic decision problem fairly easily, as in table 2.12. Table 2.12: Matching pennies as a decision problem pa pb a \\(0\\) \\(1\\) b \\(1\\) \\(0\\) And table 2.12 is the familiar problem Death in Damascus from Gibbard and Harper (1978). Let’s do one last one, starting with the familiar game Battle of the Sexes. Row and Column each have to choose whether to do R or C. They both prefer doing the same thing to doing different things. But Row would prefer they both do R, and Column would prefer they both do C. (The original name comes from a version of the story where Row and Column are a heterosexual married couple, and Row wants to do some stereotypically male thing, while Column wants to do some stereotypically female thing. That framing is tiresome at best, but the category of asymmetric coordination games is not, hence my more abstract presentation.) So table 2.13 is one way we might think of the payouts. Table 2.13: A version of battle of the sexes. R C R \\(4, 1\\) \\(0, 0\\) C \\(0, 0\\) \\(1, 4\\) As it stands, that’s not a symmetric game. But we can make it a symmetric game by relabeling the choices. Let option A for each player be doing their favored choice, and option B be doing their less favored choice. That turns table 2.13 into table 2.14. Table 2.14: Battle of the sexes, relabeled. A B A \\(0, 0\\) \\(4, 1\\) B \\(1, 4\\) \\(0, 0\\) After making that change, change column’s payouts so that it is a demonic game. The result is table 2.15 Table 2.15: A demonic version of battle of the sexes. A B A \\(0, 1\\) \\(4, 0\\) B \\(1, 0\\) \\(0, 1\\) Finally, replace Demon’s choices with states generated by (probably accurate) predictions, to get the decision problem in table 2.16. Table 2.16: A demonic decision problem based on battle of the sexes. PA PB A \\(0\\) \\(4\\) B \\(1\\) \\(0\\) That decision problem is the asymmetric version of Death in Damascus from Richter (1984). The point of this section has not just been to show that we can turn games into decision problems by treating one of the players as a predictor. That’s true, but not in itself that interesting. Instead I want to make two further points. One is that most of the problems that have been the focus of attention in the decision theory literature in the past couple of generations can be generated from very familiar games, the kinds of games you find in the first one or two chapters of a game theory textbook. And the generation method is fairly similar in each respect. The second point is that most of the simple games you find in those introductory chapters turn out to result, once you transform them this way, in demonic decision problems that have been widely discussed. But there is just one exception here. There hasn’t been a huge amount of discussion of the demonic decision problem you get when you start with the game known as stag hunt. I’ll turn to that in the next subsection. In later parts of the book, I’ll be frequently appealing to decision problems that are generated from other games that have been widely discussed by economic theorists. Most of these discussions are not particularly recent; the bulk of the work I’ll be citing is from the 1980s and 1990s, and I don’t take myself to be making a significant contribution to contemporary economic theorising. But what I want to point out is that there is a vast source of examples in the economic theory literature that decision theorists could be, and should be, discussing. And I’ve spent so long here on the translation between the two literatures in part because I think there are huge gains to be had from bringing these literatures into contact. 2.3.3 An Indecisive Example This subsection is mostly going to be talking about games that are commonly known as stag hunts. Brian Skyrms has written extensively on why stag hunts are philosophically important (Skyrms 2001, 2004), and putting them at the center of the story is one of several ways in which this book is following Skyrms’s lead. Stag hunts are symmetric two-player two-option, simultaneous move games. So they can be defined by putting constraints on the values in 2.5. In this case, the constraints are \\(x &gt; z\\) \\(w &gt; y\\) \\(x &gt; w\\) \\(z + w &gt; x + y\\) The name comes from a thought experiment in Rousseau’s Discourse on Inequality. They were perfect strangers to foresight, and were so far from troubling themselves about the distant future, that they hardly thought of the morrow. If a deer was to be taken, every one saw that, in order to succeed, he must abide faithfully by his post: but if a hare happened to come within the reach of any one of them, it is not to be doubted that he pursued it without scruple, and, having seized his prey, cared very little, if by so doing he caused his companions to miss theirs.  (Rousseau 1913, 209–10) Normally, option A is called hunting, and option B is called gathering. The game has two equilibria - both players hunt, or both players gather. So it’s unlike prisoners’ dilemma, which only has one equilibria. And the more cooperative equilibria, where both players hunt, is better. But, and this is crucial, it’s a risky equilibria. To connect it back to Rousseau, the thought is that the players would both be better off if they both cooperated to catch the stag (or deer in this translation). But cooperating is risky; if the players do different things, it is best to go off gathering berries (or bunnies) on one’s own than trying in vain to catch a stag single-handed. And that’s what we see in the game. The first two constraints imply that the game is in equilibrium if the players do the same thing. The third constraint says that if they both hunt, option A, they are better off than if they both gather, option B. But the fourth constraint codifies the thought that this is a risky equilibrium. Even though the equilibrium where everyone hunts is better, there are multiple reasons we might end up at the equilibrium where everyone gathers. One reason for this is that the players might want to minimise regret. Each play is a guess that the other player will do the same thing. If one plays A and guesses wrong, one loses \\(w - y\\) compared to what one could have received. If one plays B and guesses wrong, one loses \\(x - z\\). And the last constraint entails that \\(x - z &lt; w - y\\). So playing B minimises possible regret. Second, one might want to maximise expected utility, given uncertainty about what the other player will do. Since one has no reason to think the other player will prefer A to B or vice versa - both are equilibria - maybe one should give each of them equal probability. And then it will turn out that B is the option with highest expected utility. Intuitively, B is a risky option and A is a safe option, and when in doubt, perhaps one should go for the safe option. There are other arguments too for choosing to gather rather than hunt. To use a notion from Skyrms, gathering has a larger basin of attraction than hunting, and its basin of attraction includes the midpoint of the probability space. Those two motivations, plus the two from the previous two paragraphs, give us four motivations for gathering. If one wants to motivate gathering in general, it is actually important to get clear on which of the four one takes to be most important. Because while they are equivalent in the two option game (or problem), they are not in general equivalent. Indeed, all four come apart very soon once more options get added. Since I’m not defending gathering, I think decision theory should say either option is acceptable in stag hunts, I’m not going to continue exploring this line. But there are some interesting technical questions along these paths. We can turn stag hunt into a decision problem by replacing the other player with Demon in a way that should be familiar by now. The result is the decision problem in table (tab:stag-decision). Table 2.17: A demonic decision problem based on stag hunt. PA PB A \\(x\\) \\(y\\) B \\(z\\) \\(w\\) In order to have less algebra, I’m going to often focus mostly on a particular version of this decision, with the values shown in table ??. But it’s important that the main conclusions will be true of all decision problems based on stag Hunt. Table 2.18: A particular version of a demonic decision problem based on stag hunt. PA PB A 10 0 B 6 8 These kinds of cases are important in the history of game theory because they illustrate in one game the two most prominent theories of equilibrium selection: risk dominance and payoff dominance (Harsanyi and Selten 1988). Risk dominance recommends gathering; payoff dominance recommends hunting. And most contemporary philosophical proponents of decisive decision theories (in the sense of decisiveness described back in section 1.3) fall into one of these two camps. In principle, there are three different views that a decisive theory could have about stag decisions: always hunt, always gather, or sometimes do one and sometimes the other. A decisive theory has to give a particular recommendation on any given stag decision, but it could say that the four constraints don’t settle what that decision should be. Still, in practice all existing decisive theories fall into one or other of the first two categories. One approach, endorsed for rather different reasons by Richard Jeffrey (1983) and Frank Arntzenius (2008), says to hunt because it says in decisions with multiple equilibria, one should choose the equilibria with the best payout. Proponents of Evidential Decision Theory also say to hunt in these situations, because the all-hunt outcome is better than the all-gather outcome, and it doesn’t even matter whether these are equilibria. Another family of approaches says to always gather in stag decisions. For very different reasons, this kind of view is endorsed by Ralph Wedgwood (2013), Dmitri Gallow (2020) and Abelard Podgorski (2022). These three views differ from each other in how they motivate gathering, and in how they extend the view to other choices, but they all agree that one should gather in any stag decision. I’m going to argue that all of these views are mistaken. Decision theory should not say what to do in these cases - either choice is rational. Now I should note here that I’m slightly cheating in setting out the problem this way. The theory I defend says that in any decision problem like this with two equilibria, either choice can be rational. And that includes games like, say, the one in ??, where everyone I mentioned in the last few paragraphs would agree that A is the uniquely correct choice. Table 2.19: A multiple equilibrium decision problem that is not a stag hunt. PA PB A 10 0 B 2 4 I certainly don’t want to lean too hard on the intuition that either option is rational in a stag hunt—though I do in fact think that it’s intuitive that either option is rational in a stag hunt. But if we were just leaning on intuitions, then this last example would be devastating to my theory, since it really isn’t particularly intuitive here that either option is rational. Thankfully, the argument, which I’ll set out in some detail in chapter 4, doesn’t appeal to these kinds of intuitions. Still, I think it’s useful to focus on stag hunts because, as Skyrms shows, they are so philosophically important. And they will be my canonical example of a problem where the right decision theory is Indecisive. 2.3.4 An Example of a Dilemma Rock Paper Scissors 2.4 Plan for the Book NOT COMPLETE Talk about appendices Talk about each chapter This is literally the last thing to write "],["defensive.html", "Chapter 3 Why So Defensive? 3.1 Decision Theory and Making Decisions 3.2 Why Do Decision Theory 3.3 Why Do Ideal Decision Theory 3.4 Why Not Proceduralism", " Chapter 3 Why So Defensive? I’m arguing for causal ratificationism. And much of the argument will come in the next three chapters, when I argue in turn against three of the component parts of of proceduralism. But in this chapter I want to first address an argument that proceduralism must be right, because only procedural theories can deliver what decision theory promises: a rule for making decisions. And the main argument of this chapter is going to be that decision theory cannot, and should not, be in the business of providing such a rule. Such a rule would have to be sensitive to resource constraints, and this kind of sensitivity isn’t compatible with doing the kind of theorising that decision theorists do. 3.1 Decision Theory and Making Decisions I’ll get to why I think decision theory isn’t, and shouldn’t be, used to help humans make decisions. First I want to argue against something that is perhaps less widely believed, but it probably more plausible: that decision theory will be a helpful way for machines who are not subject to serious resource constraints to make decisions. We are, collectively, currently engaged in a massive project of making machines that make decisions, from ‘smart’ thermostats to self-driving cars. Now one might have hoped that decision theory would have something useful to contribute to this project. That hope I think can be realised, but it’s complicated. One might have further hoped that decision theory would be helpful in a way that only a proceduralist theory can be helpful, by providing an algorithm to program into the machines. And that hope that some might have won’t be, and shouldn’t be, met. That’s because sometimes we want the machines to be irrational. Here is one simple case of this, based in part on David Lewis’s work on nuclear deterrence (Lewis 1989), and in part on Dr. Strangelove (Kubrick 1964). Chooser is President of a relatively small country. Due to an unfortunate machine translation incident with their larger neighbor during widget tariff negotiations, the neighbor has become an enemy. (The fact that they are called Enemy should have been a clue that this would happen, but Chooser has the worst advisors and no one noticed this until it was too late.) And Enemy now plans to express their displeasure by launching a nuclear missile at chooser’s largest city. Chooser doesn’t have many ways to respond to this; any normal attempt at retaliation would just launch a larger war that would go very badly for Chooser’s country. Fortunately, Chooser’s military has just developed a doomsday device. If launched, the doomsday device will kill everyone in Enemy’s country. And Enemy is smart enough, or at least self-interested enough, to not do anything that will lead to the doomsday device being launched. Probably. Unfortunately, the doomsday device will not just kill everyone in Enemy’s country, it will also kill everyone in Chooser’s country. Fortunately, Chooser also has the ability to tie an automated launcher to the doomsday device, so it will launch if any nuclear missile hits their major city. And they have the ability to let Enemy know that they have tied an automated launcher to the doomsday device. So they can make a very credible threat to Enemy. If it is likely enough that Enemy will back down when threatened this way, Chooser should install the automated launcher. And, and this is very important, they should make sure Enemy knows that they have done so. Even if there is some small probability \\(\\varepsilon\\) that Enemy will launch anyway, if \\(\\varepsilon\\) is small enough, and the probability of having everyone in the largest city killed high enough, it is a risk worth taking. I’d originally thought of making the doomsday device kill not just everyone in Chooser’s country and Enemy’s country, but everyone in the world. (As was the case for the doomsday device in Dr. Strangelove.) But this complicates the decision making in ways I’d rather avoid. For one thing we have to account for the loss of future generations. For another, as Jonathan Knutzen (2022) points out, we have to account for the loss of humanity in general, on top of the loss of all those individual humans. Maybe there is no realistic probability of failure small enough that this could be a reasonable risk. But there surely is a probability of failure small enough that the risk of losing a whole country is worth trading off against the certainty of losing the largest city. And that’s the risk I’m asking Chooser to take in this particular example. And I think in the right circumstances, it’s a risk to take. But now change the example in a few ways. Chooser still has the doomsday device, but they don’t have the automated launcher. Fortunately, Enemy is now blessed, or cursed, with a Demon, who can predict with very high probability how Chooser will react if the largest city is destroyed. In particular, the Demon can predict, with very high probability, whether Chooser will react by launching the doomsday device, killing everyone in both countries. Unfortunately, the Demon seems to have predicted that Chooser will not do that, because the nuclear missile is now headed towards the largest city. What should Chooser do? I think it’s very plausible that Chooser should not respond by launching the doomsday device. Even if Chooser wants to punish Enemy country for launching the nuclear missile, which is a reasonable enough wish, the punishment would not be proportionate, and the damage to Chooser’s own citizens would be intolerable. If Chooser’s only options are the doomsday device or nothing, Chooser has to do nothing. Or so I think; I’ll just note that this is an appeal to intuition about a case and that some people may feel differently. But let’s explore what happens if you agree that it would be wrong to kill everyone in two countries to try and prevent a nuclear missile launch that’s already happened. One thing that follows from this view about Chooser is that rational choice is not the same thing as the choice a well designed machine would make. And, conversely, a well designed machine will not do just what the rational choice is. We just said that the right way to program the machine, if it is available, is to launch the doomsday device as soon as the nuclear missile is detected. But Chooser, if they are rational, will not launch the doomsday device in response to the nuclear missile. This is a counterexample to Functional Decision Theory (FDT), which says that the rational choice in a situation is the manifestation in that situation of the optimal algorithm (Levinstein and Soares 2020). The optimal algorithm is the one the machine runs: automatically launch the doomsday device. But that’s not what is rational. The problem here is that launching the doomsday device is what game theorists call a non-credible threat. And you can’t make a non-credible threat credible by loudly insisting that you’ll really really do it this time, or even that it would be the rational thing to do. There is another problem for FDT concerning pairs of cases. Change the second example so that Enemy’s Demon is actually not very reliable. In this variant, they are better than chance, but not a lot better. Now Chooser certainly wouldn’t set up the doomsday machine to automatically launch; the risk of a false positive is too high. So FDT says that in the variant where chooser has to decide what to do after learning the launch was made, chooser will do nothing. And while this is the right thing to do, it is made for the wrong reasons. Once Enemy has launched the missile, Chooser’s best estimate yesterday of how reliable Demon in general was becomes irrelevant to what chooser should do. But according to FDT it could be decisive. So if decision theory is relevant to building machines that make decisions, it’s not because the right decision theory should be built into those machines. And hence it’s not because decision theory must be proceduralist in order to make it possible to build it into these machines. It’s rather because the people who make the machines face a very hard decision problem about what kinds of machines to build, and decision theory could be relevant to that problem. But how is decision theory relevant to that problem, or indeed any problem? The next section looks at that question in more detail. 3.2 Why Do Decision Theory What are we trying to do when we produce a decision theory? I think some of the disputes within the field come from different theorists having different motivations, and hence different answers to this question. My answer is going to be that decision theory plays a key role in a certain kind of explanatory project. And I think defensivism is well suited to play that role. But to see what I mean by playing a key role in an explanatory project, it helps to compare that with other possible views about the aim of decision theory. One thing you might hope decision theory would do, and certainly one thing students often expect it will do, is provide advice on how to make decisions. I think decision theory is very ill-suited to this task, and it shouldn’t really be the aim of the theory. The primary reason for this is that in any real life situation, the inputs are too hard to identify. To use decision theory as a guide to action, I need to know the utility of the possible states. And I need to know not just what’s better and worse, but how much they are better or worse. At least speaking for myself, the only way I can tell the magnitudes of the differences in utilities between states is to ask about various gambles, and think about which of them I’d be indifferent between. That’s to say, the only way I can tell that the utility of A is half way between that of B and C is to ask whether I’d be indifferent between A and a 50/50 chance of getting B or C. So I have to know what decisions I’d (rationally) make before I can work out the utilities. And that means I have to know what decisions to make before I can even apply decision theory, which is inconsistent with thinking that decision theory should be the guide to what decisions to make. This isn’t such a pressing problem when decisions can be made using purely ordinal utilities, but those cases are rare. So in general there is little use for decision theory in advising decisions. A somewhat better use for decision theory is in evaluation. Using decision theory, we can look at someone else’s actions and ask whether they were rational. This is particularly pressing in cases where the person has harmed another, perhaps due to possible carelessness, or in defense of another, and we’re interested in whether their actions were rational. Now one immediate complication in these cases is that we don’t know the actor’s value function. Even if the action doesn’t maximise value as we see it, we don’t know whether they have a different value function (perhaps one that puts low weight on harms to others), or they were doing a bad job maximising value. We don’t know whether they were a knave or a fool. But it’s often charitable to assume that they do have a decent enough value function, and we can ask whether what they were doing was rational if they did indeed have a decent value function. This is a task decision theory is useful for, but it alone wouldn’t justify the existence of books like this one. For one thing, the theory of how to act around Demons isn’t usually relevant to whether an act was careless, or a permissible kind of self/other-defense. It is sometimes relevant. Sometimes we should think game-theoretically about whether a person was acting properly, and that will bring up issues that are similar to issues involved in making decisions around Demons. But usually the kind of decision theory we need in these cases is fairly elementary. A bigger problem is that rationality is too high a bar in these cases. (I’m indebted here to Jonathan Sarnoff.) Imagine that D puts up a ladder somewhat sloppily, and it falls and injures V. The question at hand is whether D is morally responsible for the injury to V due to their carelessness. Ladders are tricky things, and sometimes one can take reasonable precautions and bad things happen anyway. Sometimes it is correct to attribute an injury to bad luck even if a super-cautious person would have avoided it. We aren’t in general obliged to take every possible precaution to avoid injuring others. (If we were, we wouldn’t be able to go out in public.) So what’s the test we should use for whether this particular injury was just a case of bad luck or a case of carelessness? It is tempting to use decision theory here. The injury is a case of bad luck iff it was decision-theoretically rational for D to act as they did, assuming they had a decent value function. The problem is that this is a really high bar. Imagine that D did what any normal person would do in setting up the ladder, but there was a clever way to secure it for minimal cost that D didn’t notice, and most people wouldn’t have noticed. Then there is a good sense in which what D did was not decision-theoretically rational; the value maximising thing to do was the clever trick. But we don’t want people to be morally responsible every time they fail to notice a clever option that only a handful of people would ever spot. And this is the general case. Decision-theoretic rationality is a maximising notion, and as such it’s a kind of hard norm to satisfy. We don’t want every failure to satisfy it in cases of unintentional injury to others, or intentional injury to others in the pursuit of a justifiable end like self-defence, to incur moral liability. So this isn’t actually a place where decision theory is useful. And if decision theory isn’t useful in these cases, then it’s value as an evaluative tool is somewhat limited. We can still use it for going around judging people, and saying that was rational, that was irrational. And that’s a fine pastime, being judgmental can be fun, especially if the people being judged are in charge of institutions we care about. But we might hope for a little more out of our theory. A third role for decision theory is in predicting what people will do. Sometimes we know people’s incentives well enough to be able to predict that they will act as if they are rational. And at least sometimes, that can lead to surprising results. I’ll talk through one case that I find surprising, and I suspect other philosophers will find surprising too. Chooser runs a televised rock-paper-scissors tournament. Ratings are fine, but Chooser is told by the bosses that what the audience really likes is when rock beats scissors. The audience doesn’t think rock-paper-scissors is really violent enough, and the implicit violence of rock smashing scissors is a help. So Chooser is thinking about generating more outcomes where rock beats scissors. And their plan is to make a cash payoff to players every time they successfully play rock, on top of the point they get for winning the game. Chooser’s hope is that the game payoff will change from the standard payoff table, as shown in table 3.1, to table 3.2. Table 3.1: Rock-Paper-Scissors Rock Paper Scissors Rock 0, 0 -1, 1 1, -1 Paper 1, -1 0, 0 -1, 1 Scissors -1, 1 1, -1 0, 0 Table 3.2: Rock-Paper-Scissors with Bonus for Rock Rock Paper Scissors Rock 0, 0 -1, 1 2, -1 Paper 1, -1 0, 0 -1, 1 Scissors -1, 2 1, -1 0, 0 They turn to their resident decision theorist to ask how much this will improve ratings. If you haven’t worked through this kind of problem before, it’s actually a fun little exercise to work out what the effect of this change will be. Because the disappointing news they are going to get from the decision theorist is that this move will backfire. After the change the rock-scissors combination will occur less often than it did before the change. In the original game, it’s pretty clear what the unique equilibrium of the game is. Each player plays each option with probability \\(\\frac{1}{3}\\). If either player had any deviation from that, then they would in the long run be exploitable. So that’s what they will do, over a long enough run. And that means that a combination where one player chooses Rock and the other chooses Paper will occur in \\(\\frac{2}{9}\\) of games. But what’s the equilibrium of the new game? It’s symmetric, each player uses the same mixed strategy. And in that mixed strategy, a player chooses Paper with probability \\(\\frac{5}{12}\\), Rock with probability \\(\\frac{1}{3}\\), and Scissors with probability \\(\\frac{1}{4}\\). So the combination where one player chooses Rock and the other chooses Paper will occur in \\(\\frac{1}{6}\\) of games, a considerable reduction from what it previously was. It is very easy to share the intuition that if you reward a certain kind of behavior, you’ll see more of it. (Some politicians, whose knowledge of economics starts and ends with supply-and-demand graphs, rely on nothing but this intuition in economic policy making.) But that intuition doesn’t always work in the context of competitive games. Here rewarding Rock doesn’t result in any change to how often Rock is played, but does result in a reduction of how often one plays the strategy that Rock defeats. What it actually incentivises is behavior that is outside this Rock-Scissors interaction, i.e., Paper. Now this doesn’t require a huge amount of decision theory to work out - it’s pretty simple linear algebra. But the intuition that rewarding a kind of behavior causes it to be more common is widespread enough that I think a theory that predicts it won’t happen isn’t completely trivial. So cases like these are cases where, I think, decision theory has a useful predictive function. And this means it has practical advantages to the institutional designer, i.e., the chooser in this case. Knowing a bit of decision theory will tell them not to literally waste their money on this plan to reward players who win playing rock. Does it also have practical advantages to the players? Perhaps it has some, though it’s a little less clear. After all, if every other player finds the new equilibrium quickly enough, then the expected return of each strategy in a given game will be equal. Decision theory itself says that in a particular play it doesn’t matter what a player does. So it kind of undermines its own claim to being practically significant to the players. But this shouldn’t reduce how useful the theory is at predicting how players will react to changes in the institutional design, and hence how valuable the theory could be to institutional designers. But while decision theory can be predictively useful, the main role it plays is in explaining human behavior. Think about the explanation George Akerlof offers of why used cars lose value so quickly, i.e., why people don’t pay nearly as much for lightly used cars as they play for new cars (Akerlof 1970). Or think about the explanation Michael Spence offers for why employers might pay more to hire college graduates even if college does not make employees more productive (Spence 1973). In each case carefully thinking about the decision problem each actor faces can give us a story about how behavior that looks surprising at first actually makes sense. The point is not that these explanations always work. Both of them make substantive assumptions about the kind of situation that actors are in. I’m inclined to think that the assumptions in Akerlof’s model are close enough to true that his explanation works, and the ones in Spence’s model are not. But whether you think that’s true or not, what you should think is that models like these show how decision theory can play a role in simple but striking explanations of otherwise mysterious behaviour. A key assumption in each such explanation is that actors are basically rational. Or, at least, that their behaviour is close enough to what it would be if they were rational that rationality is a good enough assumption for explanatory purposes. A long tradition in philosophy of economics is that this is a fatal weakness in these explanations. After all, we know that people are not in fact perfectly rational. But I’m inclined to think it is a strength, in fact an important strength of decision theoretic explanations of behavior. The fact that people are not perfectly rational does not mean that we cannot explain their behaviour using models that assume rationality. All that we need for that is that in a particular situation, the behaviour is as it would be if they were rational. And that can be true in certain domains. For example, people who prefer vanilla ice cream to strawberry ice cream buy more vanilla ice cream than strawberry ice cream. Now wheeling out a belief-desire model of action, combined with an assumption that ice-cream purchases are made by practically rational actors, to explain that pattern of purchases would be overkill. But it wouldn’t be wrong. In some cases people collectively do act as they would if they were all rational. Not in all cases, of course, but in some. And to tell whether we are in such a case, we have to look. To know whether people are acting rationally, we sometimes need to have a sophisticated theory of decision. At first glance, it might look irrational to have a very strong preference for new cars over lightly used cars. It takes some work to see that it might be, as Akerlof argued, a rational reaction to epistemic asymmetries. This work is a project that decision theory can contribute to, and indeed has contributed to. The project of at least trying to see how surprising behaviour might be rational, a project which decision theory has a key role in, is valuable for two reasons. One reason is epistemic. It’s really easy to fall into thinking that certain behaviour is the result of a bias, and not even look for possible rational explanations of it. This is what Brighton and Gigerenzer call the ‘bias bias’ (Brighton and Gigerenzer 2015). You don’t even have to posit a bias in favor of explanations in terms of irrationality to get this result. Often the explanation in terms of irrationality is easier. It’s much easier to say that people have an irrational attachment to new cars than to build a model of rational choice under epistemic asymmetry that explains the behaviour. And obviously its easier to settle for easy explanations. A commitment to looking for rational explanations of behaviour is a useful practice because it makes the researcher not settle for simple explanations. It might be that on a particular occasion the simple explanation is right, and people are just being irrational. But it is often a good use of time to at least look at what the best rational explanation is, and see if it is as plausible as the best irrational explanation. Another reason is moral. There is a kind of respect involved in treating people as rational, or at least taking as a live option that people are acting rationally unless there is fairly strong reason to believe they are not. And we should show this kind of respect to other humans. And explanations of behaviour in terms of rational choice typically have the advantage that they make sense not just to the theorist, but to the person actually carrying out the behaviour. They allow for at least the possibility of the theorist and the person being theorised about to understand the behaviour in the same way. And that’s a kind of equality that we should value. The upshot of these considerations is that we should try to see how surprising behaviour could be rational. Rather than seeing someone as incompetently trying to carry out our ends, we should consider the possibility that behaviour we find surprising is the result of differences in what evidence is available, or in what values the actors have. Making sure we at least check what an explanation in terms of rational choice would look like is a useful heuristic because it sometimes turns up surprising and plausible models. It’s an empirical question whether it is an efficient heuristic. I suspect it is, but maybe there are other heuristics that more efficiently lead to plausible models. Even if that were so, I would still think that it would be good to start by looking for rational choice models. That’s because the moral reasons in favor of looking for these kinds of models, or for these kinds of explanations, would be decisive. That’s what I think the primary purpose of decision theory is. It’s part of the project of trying to explain surprising aspects of human behaviour in terms of rational choices by people with different amounts of evidence, and different values. And since that’s a very valuable project, I think decision theory is valuable, at least insofar as it contributes to the project. For what it’s worth, I think the kind of decision theory I’m defending, where the central principle is that choices must be defensible, has a much better track record of contributing to rational explanations of surprising behaviour than do its rivals. I don’t know of real world situations in which we see Evidential Decision Theory play a particularly useful role in explaining what people do. Are there any papers based on EDT that as good as Akerlof’s original paper on the market for lemons? Even if there are no such papers, it is possible that there are institutional reasons for this. Maybe not enough economists or political scientists get taught EDT, or maybe malicious journal editors conspire to not publish papers using models based on EDT. But if the role of decision theory is, as I’ve argued, to contribute to these kinds of explanations, it would be useful to see how much of a contribution to rival philosophical theories of decision could actually make. 3.3 Why Do Ideal Decision Theory The previous section was on why philosophers should care about decision theory. But what I’m doing in this book isn’t just decision theory, it’s a very specific kind of decision theory. What I’m doing might be called ideal decision theory. It’s the theory of how idealised agents make decisions. And I’m going to appeal to those idealisations a fair bit in what follows. This section is about why we should care about such an idealised theory, and in particular about why a ratificationist decision theory should care about it. One lesson of cases like Salesman is that every theory of idealised decision making needs to be complemented with a theory of non-ideal decision making. But this isn’t the only lesson one could take from the example. Another lesson could be that ideal decision theory is a pointless enterprise. It should not be supplemented by non-ideal decision theory, but replaced. I don’t think that’s right, but it takes some argument to say why it isn’t right. That longer argument will come in chapter 5, but for now I’ll just say what positive role I think ideal decision theory has to play. Let’s start with some things that ideal theory cannot do. It can’t give people a target they should approximate. That’s because the following is a very bad argument. The ideal is X. So, Chooser should be as much like X as they can be. We know that isn’t right for reasons set out by Lipsey and Lancaster (1956). If one can’t be like the ideal, it is often best to do other things that the ideal chooser would not do to offset these failings. Here’s one simple example. The ideal chooser, in decision theory, can do all the reasoning that is needed for a problem instantaneously. So it’s a bad idea for them to stop and have a think about it before making a big decision. Since they have thought all the thoughts that are needed, that would just be a waste of time. But it’s often a very good idea for Chooser to stop and have a think about it before making a big decision. Not doing that, in order to be more like the ideal, is a mistake. The following argument isn’t as bad, but it isn’t right either. The ideal is X. Chooser’s situation is approximately ideal. So Chooser should do approximately X. The situations where this fails are a bit more contrived than the situations where the previous argument failed, at least for typical individuals. But here the details of Lipsey and Lancaster’s argument matter. At least when Chooser is designing institutions, like market structures or taxation systems, it turns out to very often be the case that the the second best solution looks dramatically different to the best solution. And someone who is approximately ideal might only be able to find the second best solution, not the best one, in a reasonable time. So it’s possible that someone with very mild computational limitations to do something very different from what the ideal agent would do, and yet be acting optimally given their limitations. The following two arguments, however, are good arguments. And the two main uses of ideal decision theory are related to these two good arguments. The ideal is X. The differences between Chooser’s situation and the ideal are irrelevant. So Chooser should do X. Ideal theory can provide advice, in situations that are like the ideal in suitable ways. It isn’t trivial for the ideal theory to provide such advice though. The second premise is often very hard to justify. But in some cases it is not that hard—the computations that have to be made are easy, and the stakes are high, so it is worth spending the resources to make all the computations. And in those cases, we expect Chooser to act like the ideal. “Expect” here has both a normative and a descriptive meaning, and let’s make the latter of those explicit with another good argument that uses ideal decision theory. The ideal is X. The differences between Chooser’s situation and the ideal are irrelevant. So, Chooser will do X. If we interpret ‘situation’ in premise 2 to include the fact that Chooser is (approximately) rational, then this is a good argument too. And when we have an argument like this, we can use it to predict what Chooser will do, and explain what Chooser has done. This kind of argument can be used in explanations that have the structure Michael Strevens (2008) argues that explanations involving idealisations always have. They include a model saying what would happen in idealised situations. And they include a premise that doesn’t just say that the real situation is close to the ideal, but that the differences don’t matter for the purposes of what we’re trying to explain. That kind of structure covers simple explanations using the ideal gas law you might learn in introductory chemistry, and it also covers explanations using ideally rational agents. What we need for, e.g., Akerlof’s explanation of used car prices to work is not that everyone in that market is perfectly rational, but that they are close enough to rational for the purposes of predicting how they will behave in the used car market. That’s important because participants in the used car market are not perfectly rational; they might not even be close to it. But just as real molecules can be modeled by things that are infinitely smaller than them, real buyers and sellers of used cars can be modeled by actors that are infinitely more rational than they are. And that’s the big picture project that I want this book to be contributing to. There are both epistemic and moral reasons to look for explanations of behaviour in terms of individuals acting rationally. These explanations will be idealised explanations. Idealised explanations involve describing carefully how things work in the ideal, and arguing that the differences between the ideal and the reality are unimportant for the particular thing being explained. The latter task is hard, and often not done with sufficient care, but isn’t impossible. This book contributes primarily to the former task, though especially in chapter 5 I’ll have things to say relevant to the latter task as well. David Lewis gives a similar account of the purpose of decision theory in a letter to Hugh Mellor. The context of the letter, like the context of this section, is a discussion of why idealisations are useful in decision theory. Lewis writes, We’re describing (one aspect of) what an ideally rational agent would do, and remarking that somehow we manage to approximate this, and perhaps – I’d play this down – advising people to approximate it a bit better if they can. (Lewis 2020, 432) To conclude this section on an historical note, I want to compare the view I’m adopting to the position Frank Knight puts forward in this famous footnote. It is evident that the rational thing to do is to be irrational, where deliberation and estimation cost more than they are worth. That this is very often true, and that men still oftener (perhaps) behave as if it were, does not vitiate economic reasoning to the extent that might be supposed. For these irrationalities (whether rational or irrational!) tend to offset each other. The applicability of the general “theory” of conduct to a particular individual in a particular case is likely to give results bordering on the grotesque, but en masse and in the long run it is not so. The market behaves as if men were wont to calculate with the utmost precision in making their choices. We live largely, of necessity, by rule and blindly; but the results approximate rationality fairly well on an average. (Knight 1921, 67n1) Like Knight, I think that explanations in social science can treat people as rational even if they are not, even if it would “give results bordering on the grotesque” to imagine them as perfectly rational. That’s because, at least in the right circumstances, the irrationalities are irrelevant, or they cancel out, and the “as if” explanation goes through. Now I do disagree with the somewhat blithe attitude Knight takes towards the possibility that these imperfections will not cancel out, that they will in fact reinforce each other and be of central importance in explaining various phenomena. But that’s something to be worked out on a case-by-case basis. We should not presuppose in advance either that the imperfections be irrelevant or that they will be decisive. There is one other point of agreement with Knight that I want to emphasise. If we don’t act by first drawing Marshallian curves and solving optimisation problems, how do we act? As he says, we typically act “by rule”. Our lives are governed, on day-by-day, minute-by-minute basis, by a series of rules we have internalised for how to act in various situations. The rules will typically have some kind of hierarchical structure - do this in this situation unless a particular exception arises, in which case do this other thing, unless of course a further exception arises, in which case, and so on. And the benefit of adopting rules with this structure is that they, typically, produce the best trade off between results and cognitive effort. One other useful role for ideal decision theory is in the testing and generation of these rules. We don’t expect people who have to make split-second decisions to calculate expected utilities. But we can expect them to learn some simple heuristics, and we can expect theorists to use ideal decision theory to test whether those heuristics are right, or whether some other simple heuristic would be better. This kind of approach is very useful in sports, where athletes have to make decisions very fast, and there is enough repetition for theorists to calculate expected utilities with some precision. But it can be used in other parts of life, and it is a useful role for ideal decision theory alongside its roles in institutional design (as in the rock-paper-scissors example), and in explanation (as in the used cars example). 3.4 Why Not Proceduralism Let’s take stock. This chapter has been a response to the following two kinds of worries. The best thing that decision theory could do would be to provide a procedure for making good decisions. If decision theory can’t do that, it’s a pointless activity. So far the attention has been primarily on the second point. I’ve argued that it isn’t true - that decision theory can have an important role in explanations of social phenomena even if it doesn’t provide a procedure for making good decisions. But what about the first point? Even if this is a role for decision theory, wouldn’t a procedure for good decisions be better? In some sense perhaps it would be, but there is no reason to think that anything like ideal decision theory is going to be part of such a procedure for creatures like us. For just one example, the optimal procedure in cases like Salesman will give advice that contradicts what every theory of ideal decision on the market says. In practice, the best thing a decision theory can do is be part of a broader project of helping us understand and navigate the world. To do that, it need not provide a procedure that can be followed on any given occasion, and indeed it could not do that. It is better to provide tools that can be used in one or other multi-stage process. For instance, it can provide reasons for thinking that this or that institutional design will fail or succeed in a particular way. Or it can compare heuristics that humans are capable of applying. Neither of these roles require that decision theory be proceduralist. Indeed, a non-proceduralist theory that says that we can’t predict how a certain institution will work, because it generates a decision problem for individuals with multiple solutions, might be more useful than one that posits a false certainty about what will happen. So, in short, there is no argument from the purpose to which decision theory can or should be put to the conclusion that decision theory should be proceduralist. Nothing in this chapter, however, is an argument against proceduralism. It has been a long argument against the necessity of proceduralism, but nothing more. In the next two chapters, I will argue directly that the right decision theory is not proceduralist. "],["decisive.html", "Chapter 4 Against Decisiveness 4.1 What Is Decisiveness 4.2 Six Decision Problems", " Chapter 4 Against Decisiveness At the heart of causal ratificationism is the claim that in many decision problems, there is no uniquely rational solution. And this isn’t because the options are tied. Rather, it is because each of them is acceptable once it is made. Causal ratificationism is indecisive; it doesn’t always provide a verdict on what to do. This chapter argues that the right decision theory, whatever it is, is indecisive. Or, to be a little more precise, it will be an argument that the right theory, whatever it is, is weakly indecisive. As I’ll define the terms, causal ratificationism is strongly indecisive, and the argument of this chapter won’t entail that the right theory should be strongly indecisive. In fact, in the whole book I’m not going to offer any kind of proof that the right theory is strongly indecisive. Instead, I’ll argue for a number of constraints, including weak indecisiveness, and argue that causal ratificationism is the best way to satisfy those constraints. The most distinctive of these constraints is weak indecisiveness, and the point of this chapter is to motivate that constraint. The last two paragraphs used a lot of terminology, and section ?? clarifies and defines the key terms. Then in sections 4.2–??, I’ll argue that the right theory, whatever it is, is weakly indecisive. In section ?? I’ll show how this causes problems for most existing decision theories in philosophy, and compare it to existing objections to some of those theories. Finally in section ??, I’ll discuss how these considerations relate to the defence of the view I really believe: that the right decision theory is strongly indecisive. 4.1 What Is Decisiveness I will say a decision theory is decisive iff for any decision problem, it says either: There is a uniquely best choice, and rationality requires choosing it.; or There is a non-singleton set of choices each of which is tied for being best, and each of which can be permissibly chosen. A decision theory is decisive over binary choices iff it satisfies this condition for all decision problems where there are just two choices. Most decision theories in the literature are decisive, and of those that are not, most of them are at least decisive over binary choices. I’m going to argue that the correct decision theory, whatever it is, is indecisive. It is not, I’ll argue, even decisive over binary choices. That definition, unfortunately, relies on two more terms that are not easy to define: decision problem and tie. I’ll deal with these in reverse order. For decisiveness to be anything other than a trivial truth, it can’t just be that options are tied if each is rationally permissible. If that is what it meant for options to be tied, a decisive theory would just be one that either says one option is mandatory or many options are permissible. To make decisiveness a substantive claim, a different account of what it is for options to be tied is needed. I’ll borrow a technique from Ruth (Chang2002?) to provide such an account Some options are tied iff either is permissible, but this permissibility is sensitive to sweetening. That is, if options \\(X\\) and \\(Y\\) are tied, then for any positive \\(\\varepsilon\\), the agent prefers \\(X + \\varepsilon\\) to \\(Y\\). If both \\(X\\) and \\(Y\\) are permissible, and this dual permissibility persists if either is ‘sweetened’, i.e., replaced by an option that is improved by \\(\\varepsilon\\), then they aren’t tied. My thesis, the thesis that the right theory is indecisive, is that the right decision theory says that sometimes there are multiple permissible options, and each of them would still be permissible if one of them was sweetened. I’m going to provide two notions of a decision problem, one concrete and one abstract. These will correspond to the two notions of decisiveness, weak and strong, that I’ve already mentioned. First, the abstract sense. It suffices to specify an abstract decision problem to describe the following four values. (Note that this is stipulative; I’m hereby defining what I mean by abstract decision problem.) What choices the chooser has; What possible states of the world there are (where it is understood that the choices of the chooser make no causal impact on which state is actual); What the probability is of being in any state conditional on making each choice; and What return the chooser gets for each choice-state pair. Most recent papers on decision theory do not precisely specify what they count as a decision problem, but they seem to implicitly use an account like this. It is very common in philosophical work on decision theory to see a vignette that settles nothing beyond these four things, and then the writer assumes that this or that decision theory should have something to say about the problem. (Commonly they will also make firm pronouncements about the intuitively right answer to this problem, and wield this as evidence that said decision theory is false.) So while this is stipulative, I don’t think it’s particularly distinctive; most theorists think of decision problems as something like this. But look how much it leaves out! It says nothing about what time of day it is, what the weather is, how happy the chooser is feeling (unless this impacts the returns in the relevant sense), or many other things. Maybe those matter to decision theory. Maybe the right decision theory is CDT in the seminar room, and EDT in the pub, plus I guess a classification of possible situations into being more pub-like or seminar-room-like. Then a decision problem needs a fifth clause, which specifies whether the chooser is in a pub or a seminar room. At the very least, I think we should have a language for discussing decision theory that lets CDT-in-the-seminar-room/EDT-in-the-pub be a statable theory. To that end, say a concrete decision problem is a centered world that has a chooser at its center. The centered world will determine an abstract decision problem. The function from concrete decision problems to abstract decision problems is not trivial. What, for instance, does it mean to say that these are, and these are not, the choices available to a concrete chooser? But I assume there is some function. There is no function in the reverse direction; every abstract decision problem corresponds to many, many concrete decision problems. A decision theory is strongly decisive iff it is decisive over abstract decision problems. That is, it is strongly decisive iff for any abstract decision problem, it says that either there is a unique rational choice in the problem, or that some options are tied. A decision theory is weakly decisive iff it is decisive over concrete decision problems. That is, it is strongly decisive iff for any concrete decision problem, it says that either there is a unique rational choice in the problem, or that some options are tied. A decision theory is weakly indecisive iff it is not strongly decisive, and strongly indecisive iff it is not weakly indecisive. Any strongly indecisive theory is weakly indecisive, but the converse is false. The CDT-in-the-seminar-room/EDT-in-the-pub theory is weakly indecisive. If it is presented Newcomb’s Problem, it does not issue a verdict. It says both options are rationally consistent with the abstract structure of the problem. So it is not strongly decisive, which is to say it is weakly indecisive. But it is weakly decisive. In any concrete instance of Newcomb’s Problem, the chooser is either in the seminar room (or a seminar-room-like space) or the pub (or a pub-like-space), and so there is only one rational choice. So it is not strongly indecisive. The example I’ve used so far of a weakly but not strongly indecisive theory is not, I suspect, one that will appeal to many readers. There are, however much more interesting weakly decisive but not strongly indecisive theories. They will have to wait; for the next few sections, the focus will be on strongly decisive theories, and the development of an argument against them. 4.2 Six Decision Problems The core of this chapter, and indeed the core of this book, revolves around the decision problems in tables 4.1 and 4.2. I’m going to argue that whichever moves are acceptable in one of those two problems are also acceptable in the other, for any real values of \\(x_1, x_2, x_3, x_4, e\\), and any value of \\(p \\in (0, 1)\\). The names of the problems are because they are going to be the first and sixth members of a sequence of problems that I’ll present shortly. Table 4.1: Problem 1. PU PD U \\(x_1\\) \\(x_2\\) D \\(x_3\\) \\(x_4\\) Table 4.2: Problem 6. PU PD U \\(px_1 + (1-p)e\\) \\(x_2\\) D \\(px_3 + (1-p)e\\) \\(x_4\\) Once I’ve argued that these problems should be treated the same way, I’ll then argue that no strongly decisive theory does treat them the same way, so all strongly decisive theories are false. But what are these problems? What do these tables mean? In each case, Chooser has two options: U for Up, and D for Down. There is a demon, called Demon, who is arbitrarily good at predicting Chooser’s choices, but who is not causally influenced by what Chooser does.6 Demon will select either PU, meaning they predict Chooser will play U, or PD, meaning they predict Chooser will play D. The two two-way choices produce four possible outcomes, and the payouts to Chooser in each of those four possibilities are shown in the table. So these are just the kinds of problems that are familiar in the modern decision theory literature. What’s not familiar is the claim that Problem 1 and Problem 6 should be treated the same way. The argument for that goes via four more problems, and this section introduces them. Problem 2 is just like Problem 1, except it provides more backstory about why Demon is so likely to correctly predict Chooser correctly. It is a game with two players: Chooser and Demon. Chooser’s payouts are just as before, Demon gets zero payout for an incorrect prediction, and a positive payout for predicting correctly. And Demon is a very good predictor and an expected utility maximiser, so they will almost certainly make correct predictions. But note that Demon’s payouts are not the same in each case of a correct prediction. Because Demon is playing a game not making a prediction, I’ve relabeled their moves. They are called B and C; option A will come in at the next step. Table 4.3 is the game table for Problem 2. Table 4.3: Problem 2 (table version). B C U \\(x_1, 1\\) \\(x_2, 0\\) D \\(x_3, 0\\) \\(x_4, 2\\) In standard presentations of problems like Problem 1, it is assumed that Demon moves first, but the move isn’t revealed until after Chooser moves. So while these problems are evidentially like simultaneous move games, there is a sense in which it is more accurate to represent them as game trees. So let’s include the game tree version of Problem 2 as well, as figure 4.1. Figure 4.1: Problem 2 (tree version). I don’t know of any argument for treating the table version and tree version of Problem 2 differently, so I will follow the standard assumption that these are two representations of essentially the same problem. Problem 3 is just like Problem 2, except Demon is offered an exit strategy. When Demon moves, they can now choose A, B or C. If they choose B or C, the game continues as before. Chooser is told that Demon chose B or C, but not which one they chose, and knows all the payouts, and has to choose U or D. If Demon chooses A, however, the game ends, and Chooser gets payout \\(e\\), while Demon gets payout 1. Figure ((fig:abc-v3?)) is the tree for that game. Figure 4.2: Problem 3. That would be enough to specify a game, but decision theorists typically want something more. If Demon predicts Chooser will play D, then Demon will clearly play C. But what will happen if Demon predicts Chooser will play U? In that case, Chooser will get payout 1 from either A or B, and all we’ve been told is that Demon maximises expected utility. So let’s add one specification to the game. If Demon predicts that Chooser will play U, they will play B with probability \\(p\\), and A with probability \\(1-p\\). Note that this suggests Demon might not be ideally rational. From Demon’s perspective, A weakly dominates B. But they have some probability of choosing B. If one thinks that ideal rationality requires eschewing weakly dominated options, this means Demon is not ideally rational. It was, however, never part of the story that Demon is ideal; just that they are a utility maximiser. And they can be, even if they choose B. Problem 4 is just like Problem 3, except Chooser must select before being told whether Demon has adopted the exit strategy of A. Imagine, to make it vivid, that the game-master is impatiently waiting for Demon’s selection, but Demon is stuck in their room, taking their time. Chooser has to run, so the game-master says that Chooser should write their move in an envelope. If Demon chooses A, the envelope will be burned, since it doesn’t matter . If Demon chooses B or C, the envelope will be opened and what’s written in it will be Chooser’s move. Because Chooser doesn’t know which move Demon has made, the tree (shown in figure 4.3) looks a little different. Figure 4.3: Problem 4 (tree version). Since Demon has no evidence of Chooser’s move when Demon makes their one move, and Chooser has no evidence of Demon’s move when Chooser makes their move, this is effectively a one-shot simultaneous move game. So we can represent it just as well with table 4.4 as a tree. Table 4.4: Problem 4 (table version). A B C U \\(e, 1\\) \\(x_1, 1\\) \\(x_2, 0\\) D \\(e, 1\\) \\(x_3, 0\\) \\(x_4, 2\\) Since Demon is an external force that produces states of the world which Chooser has no causal influence over, we can just as well treat Demon’s moves as states. That will get us Problem 5, as shown in table 4.5. In that problem, the three moves from Problem 4 become three states. And Chooser knows that \\(Pr(C | D) = 1, Pr(B | U) = p, Pr(A | U) = 1-p\\), and all the other conditional probabilities of states given choices are set to zero. Table 4.5: Problem 5. A B C U \\(e, 1\\) \\(x_1, 1\\) \\(x_2, 0\\) D \\(e, 1\\) \\(x_3, 0\\) \\(x_4, 2\\) In this problem, the two states A and B are separated out. But these are just two possible ways that the state PU could be realised. We could collapse these states into one, and replace the listed payouts with Chooser’s expected payouts, given that PU is realised and they make a particular choice. That will give us Problem 6, with the following familiar looking table. Table 4.6: Problem 6 (reprise). PU PD U \\(px_1 + (1-p)e\\) \\(x_2\\) D \\(px_3 + (1-p)e\\) \\(x_4\\) So those are our six problems. In the next section, I’ll argue that as theorists, we should think of all these problems the same way. For simplicity, I’m going to assume that the probability that Demon predicts correctly is 1. If you don’t want to allow that this probability can be 1 without having a causal connection, it won’t make a huge difference to make it \\(1 - \\varepsilon\\). It’s just important to note in Problems 4-6 that Demon acts as if the probability is 1. Even if there is some chance of Demon being wrong, Demon acts as if this isn’t possible. This is not a distinctive assumption; in most equilibrium concepts in game theory, an equilibrium involves the players believing that their predictions of the other players are correct with probability 1.↩︎ "],["dilemma.html", "Chapter 5 Against Rational Possibility 5.1 Four Puzzle Cases 5.2 Common Features of the Puzzles 5.3 The Possibility Assumption in Philosophical Arguments 5.4 Why Allow Dilemmas", " Chapter 5 Against Rational Possibility 5.1 Four Puzzle Cases 5.1.1 Outguess the Demon Table 5.1 is a simple case that can be used to show that causal ratificationism implies the existence of dilemmas, cases where there is no action that is ideally rational. Table 5.1: A simple example of a dilemma. PU PD U 0 2 D 1 0 Assume first, for simplicity, that Chooser can’t play any mixed strategy. Chooser simply has to play U or D, and is arbitrarily confident that Demon will guess their choice correctly. Then Chooser will regret their choice whatever they do. But ideally rational choices are not regretted as soon as they are chosen. So no choice is ideally rational. The case I’ve used here is a somewhat asymmetric version of the Death in Damascus case from Gibbard and Harper (1978). A slightly more common asymmetric version, as in Richter (1984), would have something like the following table, where there is a benefit to U over D whether Demon is correct or incorrect. Table 5.2: A version of asymmetric Death in Damascus. PU PD U 1 3 D 2 0 The cases seem to raise the same philosophical issues as far as I can see, and table 5.1 is a little simpler, so I’ll focus on it. There is, I gather, a widespread intuition amongst philosophers that decision theory should treat U and D asymmetrically in this case. I don’t really share that intuition; they both look like unhappy choices to me. And it isn’t shared by orthodox game theory. When table 5.1 is turned into a game, it becomes 5.3. Table 5.3: The game version of the simple dilemma. PU PD U \\(0, 1\\) \\(2, 0\\) D \\(1, 0\\) \\(0, 1\\) That game has a unique Nash equilibrium. In equilibrium, Row, or Chooser, plays a 50/50 mix of U and D. The asymmetry in the payouts is reflected in an asymmetry in Column, or Demon’s, equilibrium strategy. Their equilibrium strategy is to play \\(PU\\) with probability \\(\\frac{2}{3}\\), and \\(PD\\) with probability \\(\\frac{1}{3}\\). And that seems intuitively right to me; Chooser should treat the options here symmetrically, and if anyone should treat them asymmetrically, it is Demon. Thinking about the equilibrium shows us how to drop the assumption that Chooser can’t play mixed strategies, while keeping the conclusion that there are dilemmas. In 5.1 the standard setup is that Demon is an arbitrarily good predictor who is an expected utility maximiser. That settles what Demon will do in all but one case: when Chooser plays a 50/50 mix of U and D. Unfortunately, that’s exactly what Chooser will do in equilibrium. So it’s an important oversight. To fill it in, let’s stipulate that Demon will also play a 50/50 mix of \\(PU\\) and \\(PD\\) if each of them have the same expected utility. That’s enough to reinstate the dilemma. Every strategy for Chooser is a mixture7 where U is played with probability \\(p\\), and D is played with probability \\(1-p\\). If \\(p &lt; 0.5\\), then Chooser thinks Demon will play \\(PD\\), so they should definitely play U, so they regret any strategy other than U for sure. And that means they regret playing this mixture where \\(p &lt; 0.5\\). If \\(p &gt; 0.5\\), then Chooser thinks Demon will play \\(PU\\), so they should definitely play D, so they regret any strategy other than D for sure. And that means they regret playing this mixture where \\(p &gt; 0.5\\). And if \\(p = 0.5\\), then Chooser thinks Demon will play a 50/50 mixture of \\(PU\\) and \\(PD\\), in which case they think the right strategy is to play U for sure. (This is where the asymmetry in the payoffs matters.) And so they regret the mixed strategy where \\(p = 0.5\\). Whichever strategy, mixed or pure, that Chooser adopts, they regret it. So no strategy, mixed or pure, is ideally rational. It’s easy to think that if mixed strategies are allowed, then the fact that these games have equilibria means they must allow strategies that won’t be regretted. The problem with that reasoning is that it assumes more about Demon’s dispositions than is known. It isn’t stipulated that Demon is perfectly rational, or that they are an equilibrium seeker. It is just stipulated that they are an excellent predictor and a utility maximiser. And that isn’t enough to rule out dilemmas, as this example shows. I’ll have much more to say about mixed strategies as this chapter progresses. But I’ll leave them here, and turn to dilemmas that are not unique to causal ratificationism. 5.1.2 Open Ended Good Here is a very familiar example of a case where every theory says there is no ideally rational choice. Open Ended Good Chooser must pick a real in \\([0, 1)\\). The higher the number they pick, the greater their reward will be. For any \\(x \\in [0, 1)\\), if Chooser picks \\(x\\), they could have instead picked \\(\\frac{1+x}{2}\\), which would have been better. So any pick they make is bad, either after they make it (as matters for causal ratificationism), or before they make it (as matters for most other theories). So it is a dilemma. It is a dilemma that involves there being infinitely many choices. But it’s hard to see why that should make a philosophical difference. I’ll return to this point in section 5.4, but for now simply note that any argument that dilemmas are impossible on theoretical grounds must say that Open Ended Good is somehow impossible. And it’s hard to see why it would be. 5.1.3 Heaven One reason someone may worry about Open Ended Good is that it involves discontinuous payouts. The limit payout, as \\(x\\) tends to 1, of choosing \\(x\\) is 1. But the payout of choosing 1 is undefined. If we allow unlimited payouts, this discontinuity goes away. Unlimited payouts are sometimes thought to be of dubious coherence due to paradoxes like St. Petersburg, and I’m somewhat sympathetic to this line of thought. But they can be motivated, even in paradoxical situations. Heaven Chooser has died, and gone to face the last judgment. God looks over Chooser’s varied and active life, and is struggling to make a final call. He eventually says, “Look there is so much bad here I can’t just let you into Heaven. But there is so much good that I can’t just send you away either. So here’s what I’ll do. You’ll stay a while here, then off to the other place. I’ve been struggling with thinking about how long you should stay here.” And at this point, Chooser is hoping God picks a very large number. Chooser has had a bit of a look around while God was deciding. This is strictly speaking against the rules, but as we mentioned, Chooser had a varied and active life. Chooser has realised that (a) days in Heaven are good, and the goodness of them does not diminish over time, (b) the other place is considerably less good, and (c) the afterlife is infinite in duration. After a pause, God speaks again. “You know what, you decide. Pick a number, any number, any natural number that is, and you’ll spend that many days here, then off to the other place.” What should Chooser do? Again, Chooser faces a dilemma. If Chooser picks \\(n\\), then it would have been considerably better to pick \\(2n\\), or \\(n!\\), or \\((n!)!\\). Whatever choice Chooser makes, will be regretted instantly. It is worth thinking through this case a bit, to get a feel for what dilemmas are like from the inside. I think there is a common view that decision theory shouldn’t allow dilemmas because it should be practical, that it should offer advice. And saying in cases like table 5.1 that whatever one does, one will regret it, isn’t exactly advice. But in Heaven, that’s obviously the right thing to say. Or, at least, it’s obviously part of the right thing to say. And both of these insights, that everything is regrettable is both correct and incomplete, will be important in what follows. 5.1.4 The Salesman The last example I’ll discuss in this section is Salesman, the problem from section 1.2.1. The aim is to find as short a path as possible through these cities, assuming that it is possible to travel between any two cities in a straight line. Figure 5.1: The 257 cities that must be visited in the Salesman problem (reprise). This isn’t a strict dilemma; there is a shortest path. But it is going to pattern with the dilemmas in a lot of philosophically important respects. And, like with Heaven, it helps to get clear on how dilemmas work by thinking through how to solve it. Just about the least thoughtful path possible orders the cities alphabetically. The resulting tour is shown in figure 5.2. Figure 5.2: A solution to the salesman problem that orders the cities alphabetically. It’s only slightly more thoughtful to order the cities from west-to-east, as in figure 5.3, but it does reduce the length by nearly two-thirds. Figure 5.3: A solution to the salesman problem that orders the cities by longtitude. Let’s try something slightly more thoughtful. Start in an arbitrary city, I’ll use New York, and then at each step go to the nearest city that isn’t yet on the path. The result looks tour is shown in 5.4. Figure 5.4: A solution to the salesman problem that chooses the nearest remaining city. It’s better, but it still looks not so great in a few respects. For one thing, it crosses over itself too many times. For another, those long lines seem like mistakes. And they are somewhat inevitable consequences of this approach. Always choosing the nearest city will mean sometimes the path leaves a region without clearing it, and has to come back. So in this map there is a single step from Wyoming to New Jersey, as the map goes back to clean up the cities near the start that were missed. Here’s a much better idea, in two stages. For both stages I’ve used the implementation in the TSP package by Michael Hashler and Kurt Hornik, and what follows is from their description (2007). The first stage uses the farthest-insertion method. The idea is to build the path in stages. At each stage, the algorithm identifies the city that is farthest from the existing path. It then inserts that city into the path at the spot where it will make the least addition to the path length. These kind of insertion algorithms are common. What makes the farthest-insertion algorithm work well is that it forces the path to start with something like a loop around the edges of the map, and this is a generally good approach. The second stage uses the two-optimization method. This takes a completed path as input, and tries to improve it by seeing if the path would be shortened by flipping adjacent points. It does this repeatedly until it finds a local minimum. One way to turn this into an algorithm is to start with a random path. But what I did here was start with the path that a particular run of the farthest-insertion method generated. The farthest-insertion algorithm needs a start city, or it will choose one randomly. So for reproducibility purposes, I used New York again. And I set the seed for the random number generator in R to 1. And the result was the elegant path shown in figure 5.5. Figure 5.5: A solution to the salesman problem that uses two algorithms. That map has a lot of virtues. There aren’t any obvious missteps on the map, where it’s easy to see just by looking that it could be shortened. The theory behind it makes sense. It was generated very quickly. It took my computer more time to draw the map than to find the path. And it is very short compared to the other maps we’ve seen. But it isn’t optimal. I’m not sure what is optimal, but the shortest one I found after trying a few methods is shown in figure 5.6. Figure 5.6: The best solution I found to the salesman problem. This used the same idea, but with a bit more brute force. The algorithm was the same two-step approach as the last map. But I had the computer cycle through all the different possible starting cities, and varied the seed for the randomisation. (The algorithm isn’t quite deterministic, since it uses random numbers to break ties when it is asked to find the farthest city, or the shortest insertion.) And setting the start city to Dubuque, Iowa, and the seed to 33, generated that map. Finding these settings took some time, and I’m not sure it is even optimal. It would actually be helpful for the argument I’m about to make if it isn’t optimal, and the shortest path is shorter again. But let’s assume that it, or something like it, is. Because what I want us to focus on is the philosophical status of the tour in figure 5.5. Getting clear on that will be the beginning of a solution to the puzzles about dilemmas. 5.2 Common Features of the Puzzles 5.2.1 Characteristics of the Puzzles Multiple standards Stake Sensitivity Might be no available ideal play Especially if there are cognitive limits 5.2.2 Multiple Standards Tempting to have a binary split into ideal/non-ideal The big point of this chapter is that this is a bad way to do things Talk about Salesman at length, including the maps Key point: Not just quantitative differences, but qualitative differences 5.2.3 Stake Sensitivity Whether something is ideal is not stake-sensitive (unless something weird happens - e.g., me on evidence) But whether something is fine is stake-sensitive Example from open ended good Example from Salesman 5.2.4 No Ideals Foot on two kinds of dilemmas The best option might be fine. Weirich 1985 AJP says that it’s absurd to have dilemmas, then says that ideal decision is impossible in these cases! That’s what I mean by a dilemma :) 5.2.5 Cognitive Limits and Mixed Strategies If there are limits, salesman could be a dilemma I think, though I’m not relying on it, same is true for assuming can’t randomise Why say that someone can’t? Stipulation: But then could stipulate that they can’t solve salesman Realism (quote Lewis): But we can’t solve salesman. In fact, we’re better at randomising than salesman Punishment: Then we’ve given up ideals. Compare my NBA example No good argument really But it doesn’t matter, because in Outguess, if Demon does 50/50 if Chooser does 50/50, still a dilemma. 5.3 The Possibility Assumption in Philosophical Arguments A bad argument form, and why it matters 5.3.1 An Argument against EDT Open Ended Good Of course, this is a bad argument 5.3.2 A Recipe for Counterexamples Once you see the idea behind the argument in subsection 5.3.1, it’s easy to see how to construct ‘counterexamples’ to any decision theory that allows for dilemmas. For instance, here is a general purpose recipe for constructing counterexamples to most forms of Causal Decision Theory, causal defensivism included. Step One Find some game where one player has demon-like payouts, and there is no pure strategy equilibrium for the other player. By a ‘demon-like payout’, I mean that player’s payouts are all 1s and 0s, and you can sensibly interpret the 1s as situations where they correctly predicted the other player’s choice. Step Two Translate that game into a demonic decision problem, taking care to stipulate that the human player is incapable of playing mixed strategies, or that something very bad will happen to them if they do.8 Step Three Look at the pure strategies the player has, and identify the one that it would be least plausible to have one’s theory recommend. Step Four Argue, correctly, that all strategies other than the one you’ve identified are irrational according to CDT. Step Five Infer that the remaining strategy is the one that CDT recommends. Step Six Point out that it is really unintuitive that CDT would recommend that strategy, declare that this is a clear counterexample to CDT, declare victory, etc. Hopefully the discussion in section 5.3 will have made it clear that we can run this recipe against just about any theory, so it overgenerates. And not just that, we can identify the misstep - it’s step five. 5.3.3 An Instance of the Recipe One of the Frustrator examples 5.3.4 Betting Against the Demon There is a slight variation on the recipe in an interesting example due to Arif Ahmed (2014, sect 7.4.3). Here the plan at step 6 is not to argue that CDT gets the wrong result, in fact Ahmed endorses the conclusion he thinks CDT ends up with, but to argue that CDT undermines its own principles. I’m going to lean a bit on the discussion of the example in the review of Ahmed’s book by James Joyce (2016), though I end up drawing a slightly different conclusion to Joyce about the argument. And I’m going to start with a simplified version of the example, where I think it’s clearer where things go wrong, and build up to the version Ahmed gives. (I’ve also relabeled the moves, because I find the labels here more intuitive.) The simple version of Ahmed’s example, which we’ll call Betting Against the Demon, has three stages. Stage One Demon predicts whether Chooser will choose 1 box or 2 boxes at stage two. Stage Two Chooser chooses 1 box or 2 boxes. They receive $100 if Demon predicted they will choose 1 box, and nothing if Demon predicted they will choose 2 boxes, whatever they choose. This payout is not revealed to them until the end. Stage Three Chooser selects one of two bets. Bet R (for right) wins $25 if Demon predicted correctly, and loses $75 if Demon predicted incorrectly. Bet W (for wrong) wins $75 if Demon predicted incorrectly, and loses $25 if Demon predicted correctly. After this the moves are revealed, and Chooser gets their rewards. The strategic form of the game is shown in figure ??. I’m assuming Demon wants to make correct predictions, and ignoring strategies that differ only in what Chooser does in worlds that are ruled out by their Stage Two choice. (I’ll leave it as an exercise for the reader to confirm that they aren’t relevant to the analysis.) Table 5.4: The strategic form of Betting Against the Demon P1 P2 1R 125,1 -75,0 1W 75,1 75,0 2R 25,0 25,1 2W 175,0 -25,1 For Chooser, each row sets out what they do at stage two, and what they do at stage three - the number is whether they pick 1 box or 2, and the letter is whether they take bet R or W. For Demon, P1 is that they predict 1 box, and P2 is that they predict 2 boxes. It should be clear enough that there are no pure strategy Nash equilibria for this game. The best response to P1 is 2W, but the pair of 2W and P1 is not a Nash equilibrium. And the best response to P2 is 1W, but the pair of 1W and P2 is not a Nash equilibrium. But of course this game is not a strategic form game, it’s an extensive form game. Figure 5.7 is the game tree for it. Figure 5.7: The game tree for the betting against the demon example Demon moves first, but Chooser is not alerted to Demon’s move until the end of the game. So at every stage Chooser has two nodes in their information set - one for each of Demon’s possible moves. But Chooser does know their own prior move, so at stage three the information sets do not include both the top and bottom of the tree. This looks a lot like a signaling game, but there are some notable differences. There is no move from Nature here; rather Demon moves once and Chooser moves twice. And there is an information set connecting the nodes in the middle-left and middle-right of the tree. Since it is a game where Chooser moves twice, the relevant game theoretic concept to use is Bayesian Perfect Equilibrium. A strategy is defensible for Chooser iff it is part of a Bayesian Perfect Equilibrium. Since all Bayesian Perfect Equilibria are Nash Equilibria, and the game has no Nash Equilibria, there are no defensible moves. But the fact that it is a dynamic game is important to Ahmed’s argument.9 Assuming that Chooser plays a pure strategy, in equilibrium Demon knows that strategy, and Chooser knows this. So at stage three, Chooser will believe that choosing R will win $25 more or less for sure, and choosing W will lose $25 more or less for sure. So Ahmed infers, correctly, that CDT says that Chooser should not choose W. From this he infers, incorrectly, that CDT says that Chooser should choose R.10 This doesn’t follow without an extra, incorrect, assumption that CDT says that something should be chosen in this case. But in fact, since the game has no pure strategy equilibria, that isn’t true. Anyway, from that false assumption, Ahmed infers, correctly, that CDT recommends playing some strictly dominated strategy or other. And that’s incoherent, since CDT is motivated by the thought that one should never play strictly dominated strategies. I’ve simplified matters a little here, but not in a way that matters. Ahmed doesn’t say that Demon is arbitrarily accurate, but only that Demon is 80% accurate. But it’s easy enough to modify the game to accommodate this. In fact there are two ways to do so, following the two tricks I introduced in section (familiar). The Smullyan approach is to have a stage 0, where the Demon is assigned to one of two types: Knight or Knave. The former has an 80% chance of being the assignment. The Demon is told the assignment but Chooser is not. And if Demon is assigned Knave, their payouts are flipped - they get 1 if they play P1 and Chooser selects 2, or they play P2 and Chooser selects 1. The Selten approach is to have a stage 1A, between Demon and Chooser’s moves. Demon chooses P1 or P2, then at stage 1A, there is a 20% chance that Nature will reverse Demon’s choice, and the payouts will be a function not of what Demon did, but what happened after Nature modified Demon’s choice. The trees for either case are impossibly messy, and I won’t draw them. But it doesn’t matter - the equilibrium analysis of either game is exactly the same as the equilibrium analysis of the game where Demon is arbitrarily accurate, and I’ll stick with that game from now on. So does CDT recommend playing a dominated strategy? Of course not. As Joyce says, CDT is “built to guarantee” that it doesn’t recommend dominated strategies (Joyce 2016, 231). What it says, or at least what causal defensivism says, is two-fold. If Chooser can randomise, they should randomise. They should play the one and only equilibrium strategy in this game, which is a 50/50 mixture of 1W and 2W. In equilibrium, Demon will play a 50/50 mixture of P1 and P2. So at stage 3, Chooser will think it is 50% likely that Demon has ‘predicted’ correctly. (The scare quotes are because calling the output of Demon’s mixed strategy a prediction stretches language just about to breaking point.) So Chooser will be happy sticking with their strategy at stage 3 - it will have an expected return of $25, and the alternative has an expected return of -$25. If Chooser can’t randomise, then it’s a dilemma. What we say here should be similar to what EDT, or any other decision theory, says about the example in section 5.3.2. But what is that? Let’s come back to that question in a bit - first I’ll address some objections to the very idea that there could be dilemmas in decision theory. 5.4 Why Allow Dilemmas 5.4.1 Arguments Against Dilemmas Decision theory assigns values This doesn’t rule out all dilemmas And we reject the assumption Dilemmas should only be allowed in infinite cases. Why? Quote Ahmed 2012, and mock Decision theory should be guiding Reject the assumption Shouldn’ t treat every option the same This is better argument, but we can reject it. Some are fine 5.4.2 What Dilemmas are Like Be like a smart rationally constrained person Use heuristics that work much of the time, on average This might end up looking a lot like EDT Maybe this is why Newcomb’s Problem is so hard I’m assuming here that the pure strategies U and D are improper mixtures, where Chooser plays one of them with probability 1, and the other with probability 0. This assumption isn’t at all needed for the proof; but it simplifies the presentation.↩︎ I’m a bit perplexed as to why this stipulation is so widely accepted in the decision theory literature. But it usually seems to be accepted with minimal fuss. We’ll return to this point in section ??.↩︎ I’m only going to discuss what Ahmed says about CDT combined with so-called ‘sophisticated’ choice, since it’s the right way to make sequences of decisions.↩︎ Strictly speaking, what he shows is that this follows given that we are using the version of CDT described by Lewis (1981a). But he says earlier, in section 2.8 of his book, that he thinks all versions of CDT will agree with Lewis’s version about everything that is covered in the book. As he puts it, “As far as this discussion is concerned, any one of them could stand for their disjunction.” This is false, and importantly so at just this point. The version of CDT developed by Skyrms (1990) for instance would not agree with Lewis here, and would not say that any pure strategy is rational. Ahmed does discuss some of Skyrms’s earlier work, but not the version of CDT that is developed from 1990 onwards using equilibrium concepts.↩︎ "],["coherence.html", "Chapter 6 Against Coherence Norms 6.1 Coherence for the Incoherent 6.2 Coherence is a Substantive Norm 6.3 Coherence in Signaling Games", " Chapter 6 Against Coherence Norms Some say decision theory is just about doing well by one’s own lights. I follow (Comesana2020?) in saying rational action requires rational belief. Actually doesn’t quite require - Knight’s kid playing in the field is fine And I mean I think something stronger than Comesana - I mean that there isn’t anything extra good re coherence. THESES All coherence norms are wide scope. It isn’t that since Chooser thinks L is more likely than R, that Chooser should pick L over R. It is, rather, that Chooser shouldn’t both think L is more likely than R and choose R over L. If Chooser does both those things, one should change, but decision theory is silent about which it is. Some coherence norms are actually fickleness norms. Orthodox decision theory doesn’t capture fickleness in desire or, in some cases, belief. But the cases should be treated the same. We need some coherence norms to capture sure-thing, which plays a role in my theory. There is no upside to being coherent if one is irrational; coherence is a multiplier not an addition. (Maybe quote Harman on this.) Rational action does not require belief, as in the case of Knight’s child playing in the field. EXAMPLES Chooser1 has two boxes in front of them, L for Left and R for Right. A valuable good is in one of the boxes; the other box is empty. Chooser1 has to chooser L or R, and they will get what is in the box they chose. Their evidence suggests the good is more likely in L, but they believe the good is more likely in R. What should they do? Chooser2 faces the same setup as Chooser1: two boxes, a good in just one of them. But their evidence is indecisive. A rational person with that evidence could believe the good is more likely in L, but another rational person with that evidence could believe the good is more likely in R. As it turns out, Chooser2 rationally believes the good is more likely in L. What should they do? Chooser3 is just like Chooser2, in terms of (a) how the boxes are setup, (b) the evidence they have, and (c) their belief about where the good is (i.e., that it is more likely in L). But Chooser3 has a coin in their hands that they can flip. And Chooser3 has the following dispositions. Right now, they are disposed to take R. After flipping the coin, they are disposed to take L if the coin lands heads, and to take L if the coin lands tails. Is there anything wrong with Chooser3? Chooser4 is just like Chooser1; their evidence points to the good being in L. They don’t have any beliefs about where the good is more likely to be, or any credence about the location of the good. What should they do? Chooser5 is going to a dinner this weekend, and they have to specify in advance whether they want red wine or white wine. On Monday, they said they wanted red wine. On Tuesday, they changed their mind and said they wanted white. On Wednesday morning, they changed their mind again and said they wanted red. By Wednesday lunchtime, they changed back to white. There are no practical costs to this change; Chooser5 likes using the website where the wine choices are entered. Moreover, if they weren’t using that website they would be doing something of negative value, like unsuccessfully gambling on cryptocurrencies, or watching online porn, or getting into political fights on social media. Is there anything wrong with Chooser5? 6.1 Coherence for the Incoherent What is it to do best by one’s own lights If one believes p, q -&gt; -p, and q, is it best to act as if p, or as if q At best we can get rules for how to be more coherent if you are a bit coherent And that’s maybe something, but not a lot But even this requires there being a gap between coherence norms and evidence norms, and I don’t think there is 6.2 Coherence is a Substantive Norm Summarise Worsnip book Intuition about guy who punches self in head because he thinks it will mean some trivial thing he cares about is realised Intuition about guy who goes from p to p v q1, p v q2, etc, etc, all day every day Intuition that Dummett/Priest etc are, if wrong about anything, wrong about a substantive matter Worsnip response - everyone has a tendency to be coherent Response - what, and I cannot stress this enough, the f There is no way that’s a correct description of the dialethist, the intuitionist, etc 6.3 Coherence in Signaling Games So far I’ve offered the following argument against the view that decision theory is only about how people should act given their existing beliefs and desires, and has no interest in the rationality of the beliefs. The view does not make sense when applied to people whose beliefs are not just irrational, but incoherent. So the view needs a distinction between coherence norms on belief, which must be satisfied for decision theory to be applicable, and substantive norms on belief, which are irrelevant to decision theory. But thinking about heterodox logicians reveals that there is no distinction to be found here. So, the view does not ultimately make sense. Here I want to change tack and offer a direct argument, from with decision theory, for the argument that the decision-theoretic notion of rational action is sensitive to the rationality of the chooser’s underlying beliefs. The argument is going to be that the best solution to the beer-quiche game (Cho and Kreps 1987) requires that we look at the rationality of the underlying beliefs, not just at which actions flow in the right way from existing beliefs. To start, let’s translate the beer-quiche game into decision-theoretic terms, using an arbitrarily accurate demon. The problem is a little more complicated than Newcomb-like problems often are, but it should be reasonably familiar if one is used to the kind of signaling games first developed by David Lewis (1969). The game goes through the following stages. Both Chooser and Demon are informed of all the following facts, and it is made clear that they are common knowledge. Chooser is randomly assigned to one of two types, which we’ll call \\(u\\) and \\(d\\), for Up and Down. This assignment is done by a random device which has an 0.6 chance of assigning Chooser to \\(u\\), and an 0.4 chance of assigning Chooser to \\(d\\). Demon is not told of the assignment, and cannot predict how random devices work. Chooser will then make a choice of two options, which we will label \\(U\\) and \\(D\\). Demon will be told which option Chooser takes. Demon will then try to guess which type Chooser is. In making this guess, Demon will use their arbitrarily good ability to predict Chooser’s strategy. The strategy, in the relevant sense, is Chooser’s function from type assignment to choice. Chooser can randomise, so a function is a pair of probabilities - what probability of selecting \\(U\\) if they are type \\(u\\), and what probability of selecting \\(U\\) if they are type \\(d\\). Chooser gets 2 utils if Demon predicts they are type \\(u\\), and 1 util if their choice ‘matches’ their type, i.e., if they select \\(U\\) if they are \\(u\\) or \\(D\\) if they are \\(d\\). Demon gets 1 util if their guess is correct. Figure 6.1 presents the game in graphical form. Figure 6.1: A Signaling Game The game starts in the middle. Nature assigns Chooser to a type, and we move either up, if they are assigned \\(u\\), or down, if they are assigned \\(d\\). Then Chooser chooses an option. We move left if they choose \\(U\\), and right if they choose \\(R\\). Then Demon chooses, and we move up or down on the angled lines. The dotted lines around the two nodes are there because Demon doesn’t know precisely which node they are at. They know what Chooser chose, the nodes inside the dashed lines are alike in that respect. But they don’t know which type assignment was made. And then we get the payouts, using the formulae in lines 6 and 7. Note that while Demon can perfectly predict Chooser’s strategy, it doesn’t follow that they will perfectly predict Chooser’s type. This can be true even if Chooser uses a non-probabilistic strategy. In particular, it is true if Chooser adopts what’s called a pooling strategy, of playing the same option whatever type they are. If Chooser plays \\(U\\) whether they are \\(u\\) or \\(d\\), the Demon will get no information from the play, and have to use their prior credence that Chooser has an 0.6 chance of being \\(u\\). And so, it will be expected utility maximising for Demon to guess that Chooser is \\(u\\), and that’s what they will do. And the same goes for the situation where Chooser’s strategy is to play \\(D\\) no matter what. The non-pooling strategies, on the other hand, are not stable. If Chooser gives Demon information about their type, that will mean Demon is more likely to accurately guess their type. And that’s bad news for choosers who are of type \\(d\\). But since Chooser knows their type when they act, they will not perform an act that’s bad for their type. So they will not do anything other than play one of the two pooling strategies. (I’m glossing over a lot of the details here, but this is well worked out territory. See, inter alia, Cho and Kreps (1987) for the more careful version of the argument in this paragraph.) So Chooser will play a pooling strategy. But which one will they play? Playing \\(U\\) makes sense. If they are of type \\(u\\), then they will get the best possible return, so they will be happy to follow through on the strategy. And if they are of type \\(d\\), they will get a return of 2 from this strategy, and a return of 1 if they deviate from the strategy and play \\(D\\). The best solution to beer-quiche puts constraints on priors These aren’t coherence constraints in any recognisable sense But they are the kind of thing decision theory should take into account So decision theory should take substantive notions of rationality into account "],["responding-to-evidential-decision-theory.html", "Chapter 7 Responding to Evidential Decision Theory 7.1 So Why Ain’t You Rich? 7.2 To Bet the Impossible Bet", " Chapter 7 Responding to Evidential Decision Theory 7.1 So Why Ain’t You Rich? There is a familiar complaint against causal decision theory that goes back to the modern origins of decision theory in the 1970s. Here is a recent version of it due to Ahmed and Price (2012). While their version is primarily directed against proceduralist forms of causal decision theory, this particular objection does not turn on the proceduralism. If the objection works, it also works against my defensivist version of causal decision theory. (I’ve slightly changed some of the wording, but otherwise this argument is quoted from page 16 of their paper.) In Newcomb problems, the average returns to one-boxing exceed that to two-boxing. Everyone can see that (1) is true. Therefore one-boxing foreseeably does better than two-boxing. (by 1, 2) Therefore Causal Decision Theory (CDT) is committed to the foreseeably worse option for anyone facing Newcomb’s problem. Here’s what they, and many other proponents of Evidential Decision Theory (EDT) say follows from 4. The point of the argument is that if everyone knows that the CDT-irrational strategy will in fact do better on average than the CDT-rational strategy, then it’s rational to play the CDT-irrational strategy. (Ahmed and Price 2012, 17) This is what Lewis (1981b) called the “Why Ain’cha Rich” argument, and what following Bales (2018) I’ll call the WAR argument. I’m going to argue the last step of the WAR argument doesn’t follow. Or, at the very least, that proponents of EDT cannot coherently say that it follows. For there are several cases where EDT foreseeably does worse than CDT. This section will go over three of them. 7.1.1 Example One - Split Newcomb This game takes place over three rounds. At stage one, the human player chooses Play or Exit. If they choose Out, player gets 5 and demon gets 1. If they choose In, we move onto stage two. At stage two, demon chooses Left or Right, and this choice is announced. At stage three demon and the player simultaneously choose either Up or Down. Demon is very good at predicting what player’s choices will be, and indeed at stage two they were already very good at making such a prediction. And Demon wants to use these predictive powers to get as high a payoff as possible, and this is common knowledge. If Demon chose Left at stage two, stage three involves the game in table 7.1. Table 7.1: The left hand side of Split Newcomb PU PD U \\(2, 1\\) \\(4, 0\\) D \\(1, 0\\) \\(3, 3\\) But if Demon chose Right at stage two, stage three involves the game in table 7.2. Table 7.2: The left hand side of Split Newcomb PU PD U \\(12, 4\\) \\(14, 0\\) D \\(11, 0\\) \\(13, 2\\) If you’d prefer it as a game tree, it is presented in figure 7.1. Figure 7.1: Tree Diagram of the Split Newcomb Game We start at the hollow node in the middle, and Chooser (here denoted as ‘C’) either goes up by Playing, or goes down by Exiting. Then Demon moves Left or Right. Then Demon moves again, making a prediction. But this second move isn’t revealed to Chooser, which is why on either side Chooser’s nodes are in an information set. That’s to say, Chooser chooses Up or Down knowing whether Demon has gone Left or Right, but not knowing whether Demon has predicted Up or Down. And then we get the payoffs. Whether Demon goes Left or Right, the CDTer will choose Up, and the EDTer will choose Down. Either choice Chooser faces is a fairly straightforward Newcomb Problem. In both sub-games Up causally dominates Down, but Down will get a higher return if you assume, as we did assume, that demon mostly makes correct predictions. So at stage two, Demon will know that if the person facing them is an EDTer, they will get a return of 3 from Left and 2 from Right. (They’ll end up in the Down-Down cell either way.) So they will rationally choose Left. On the other hand, if the person facing them is a CDTer, they will get a return of 1 from Left and 4 from Right. (They’ll end up in the Up-Up cell either way.) So they will rationally choose Right. And everything in this paragraph can be deduced by a rational player at stage 1. So at stage one, a CDTer will know that if they Play, they expect to get 12 (the game will go Right then Up-Up), and if they Exit, they know they’ll get 5. So they’ll Play. But an EDTer will know that if they Play, they expect to get 4 (the game will go Left then Down-Down), and if they Exit, they know they’ll get 5. So they’ll Exit. The result of all this is that the CDTer will get 12, and the EDTer will get 5. So the CDTer will predictably do better than the EDTer. Indeed, the EDTer will voluntarily choose at stage one to take a lower payout than the CDTer ends up with. This seems bad for EDT, at least if we think that predictably ending up with a lower outcome is bad. Now you might object that this is because at stage two the demon chooses to treat the EDTer differently to how they treat the CDTer. I don’t really agree for two reasons, though I’m not sure either of these reasons work. (Hence the second and third examples that are about to come.) One is that the demon isn’t trying to harm the EDTer; they are just trying to maximise their return. It so happens that EDT is such an impermissive theory that it doesn’t allow for any flexibility, and the Demon, knowing this, is forced to take choices that are bad for EDT, and indeed worse for Demon than if they ended up at Right-Up-Up. But this isn’t Demon’s fault; it’s the fault of EDT being so impermissive. The other reason is that Demon does not in fact make any choices that hurt the EDTer. The EDTer should expect that Demon will in fact make such choices, in response to their theory, but that’s not quite the same thing. The only player who moves at all in the EDT version of the game is Chooser. So it’s a little hard to say this is just a case where the EDTer is harmed by the demon’s malicious choices. I think those responses work, but I’m not completely sure that they do. So let’s look at a different example, one where Demon doesn’t have these variable payouts. 7.1.2 Example Two - Coins and Signals This example is a version of a signaling game of the kind introduced by Lewis (1969). And in particular it’s a version of the broadly adversarial kinds of signaling games that are central to the plot of Cho and Kreps (1987), and which we discussed a lot in chapter (coherent). Again, it will involve three stages. At the first stage a fair coin is flipped, and the result shown to Chooser, but not to Demon. At the second stage, Chooser will choose Up or Down, and the choice will be publicly announced. At the third stage, Demon will try to guess what the coin showed. Demon knows the payoff table I’m about to show you, and is arbitrarily good at predicting Chooser’s strategy. That is, Demon can make accurate predictions of the form “If Heads, Chooser will make this choice, and if Tails, they will make that choice.” The payoffs to each player are a function of what happens at each of the three steps, and are given by the following table. Table 7.3: Payoffs for the coins and signals game. Coin Chooser Demon Chooser Payoff Demon Payoff H U H 40 1 H U T 400 0 H D H 0 1 H D T 0 0 T U H 40 0 T U T 28 1 T D H 28 0 T D T 36 1 Figure 7.2 shows game they are playing in tree form. Figure 7.2: Tree Diagram of the Coins and Signals Game Demons’s payoffs are just as you’d expect - they get rewarded iff they figure out how the coin landed. Chooser’s payoffs are more complicated, but the big thing to note is they get the biggest rewards if they manage to play Up while Demon makes an incorrect prediction. One last thing to stipulate about Demon before we analyse the game. If Demon predicts Chooser will do one thing if Heads and another if Tails, they will use the information from Chooser’s choice to make their guess about how the coin landed. But if they predict Chooser will say the same thing whether the coin landed Heads or Tails, they won’t know how the coin landed, and will flip their own coin to make a guess. So in that case it will be 50/50 whether Demon says Heads or Tails. Onto the analysis. It should be fairly clear that if the coin lands Heads, the human should say Up. The worst possible return from Up is 40, the best possible return from Down is 0. So that’s what both a CDTer and an EDTer would do, and hence what Demon would predict that they will do. So what happens if the coin lands Tails? Given Demon will predict Up if Heads, we can work out the value of Up and Down if Tails to the EDTer. If they play Up, Demon will predict that, and hence Demon will flip a coin to choose Heads or Tails. So they have a 50/50 shot at getting either 40 or 28, and so their expected return is 34. If they play Down, Demon will predict that, and hence Demon will say Tails, and they will get a return of 36. Since 36 &gt; 34, they will play Down if Tails. That’s the unique solution to the game for the EDTer. They play Up if Heads, Down if Tails. Demon can figure out that they’ll do this, so will correctly guess what the coin showed. And they will get 40 if the coin landed Heads, and 36 if it landed Tails, for an expected return of 38. What should the CDTer do? And, in particular, what should a causal defensivist do? Well, it turns out this is another problem where the theory is not decisive. Doing exactly what the EDTer does is defensible. But it’s also defensible to say Up no matter what. Let’s go over why this is defensible. The question is whether Chooser can endorse their decision to play Up no matter what after each possible result of the coin toss. They can clearly endorse it if the coin lands Heads; in that case Up strictly dominates Down, and strictly dominant are always defensible. What if the coin lands Tails? Well they think they’ll play Up. So they think the demon will flip a coin to guess in this situation. So they think the expected return of Up is 34 (like the EDTer thinks), and the expected return of Down is 32. The key difference here is that when working out the expected return of a non-chosen option, the Chooser who believes in causal defensivism does not change the expected behavior of the demon, while the EDTer does. (This disposition is why dominance reasoning works for them.) So Chooser will think that even if the coin lands Tails, they would do worse on average if they switched to playing Down if Tails. So it follows that they can defensibly play Up either way. And if they do play this, the rewards are handsome. The demon won’t have any information about the coin, so the demon will flip their own coin. So lines 1, 2, 5 and 6 of the table are all equally likely to appear. So if Chooser plays this strategy, they are equally likely to get a return of 40, 400, 40 or 28, for an overall expected return of 127. And this is much higher than the 38 the EDTer is expected to receive. By changing the payout on line 2 of the table, we can make the gap in expected returns be arbitrarily large. Now you might object that while the the causal defensivist can do better, it doesn’t follow that EDT is wrong. After all, we’ve just said here that a rival theory may do better. I don’t think that matters much. The point of the WAR is to refute a theory, and if the EDTer does foreseeably worse than one kind of Chooser who follows causal defensivism, that should be enough to refute them. But just in case you think this objection is stronger, we’ll include one last example. 7.1.3 Example Three - Coins and Newcomb This is just like Example Two, with one twist. If the game goes Tails, Down, Tails, then we don’t immediately end the game and make payouts to the players. Instead we play another game, with a familiar structure, and the payouts shown in table 7.4. As always, Demon is really good at predicting Chooser’s play, and Chooser’s payouts are listed first in every cell. (I’m not going to include a tree here, because it is more confusing than helpful.) Table 7.4: The payouts after tails–down–tails in coins and signals. PU PD U \\(20, 1\\) \\(40, 0\\) D \\(16, 0\\) \\(36, 1\\) The EDTer will think they’ll get 36 from this game, so the example will be just like Example Two. And the EDTer will play Up if Heads, Down if Tails, for an expected return of 38. But if Chooser follows causal defensivism, then they will think that if the game gets to this stage, they’ll get 20. So now they think that in the original game, Up dominates Down no matter whether the coin lands Heads or Tails. So now they will definitely play Up no matter what, and get an expected return of 127. 7.1.4 Why The Examples Matter I’ve argued against EDT elsewhere in the book, but note that this section is very much not an argument against EDT; instead, it’s part of a war on WAR. The point of the first example is that any theory whatsoever is subject to a WAR argument. That’s because for any theory whatsoever, you can construct pairs of choices like Left and Right, where the theory says to take choices that lead Demon to preferring to go Left. So for any theory whatsoever, or at least any theory that is consequentialist in the sense popularised by Hammond (1988), there is an example where the theory leads to worse returns. So any consequentialist theory is subject to an objection by WAR. It’s the paradigm of an over-generating objection. There is perhaps something a bit interesting about the second example, though it isn’t a problem especially for EDT. What makes the second example work is that Chooser is in a situation that rewards unpredictability, but EDT is decisive, and hence predictable. And any decisive theory will be subject to an WAR-style objection from cases like this. Now this isn’t part of my argument for indecisiveness, since I think WAR-style objections are bad. But following the recipe from subsection 7.1.2, it is possible to construct a case where any decisive theory will lead to predictably bad outcomes. It’s always incoherent to endorse the combination of WAR reasoning and decisive decision theories. Even though I reject both of these, I think it’s pretty clear in this case that WAR reasoning should be the first to go. And with it goes not EDT, but one of the historically key arguments for EDT. Let’s turn to a more contemporary argument for EDT next. 7.2 To Bet the Impossible Bet The Ahmed cases all go away if you focus on what’s possible at the end of deliberation, not at the start of it This might require some contextualism about ability, a la Hawthorne and Pettit, to really make stick See page 63 of Joyce book for the premise that counterfactuals about the bet must be specified in order for us to have a real bet. See also Joyce’s review of Ahmed for much more on this "],["epistemic-uniqueness-and-decisiveness.html", "Chapter 8 Epistemic Uniqueness and Decisiveness 8.1 Uniqueness and Permissivism 8.2 Chicken 8.3 Elections 8.4 Objections 8.5 Conclusion", " Chapter 8 Epistemic Uniqueness and Decisiveness 8.1 Uniqueness and Permissivism In chapter 4 I argued that decision theory is often compatible with multiple unequal solutions to games. And in chapter 6 I argued that decision theory should be sensitive to what credences the chooser should have, not just the credences they actually have. If you add to these views the philosophical view known as Uniqueness, you get something close to a contradiction. Uniqueness says, roughly, that in any situation there is a unique set of credences one should have. To keep my views coherent, I need to endorse its negation: Permissivism, which says that rationality is compatible with a number of distinct attitudes in particular situations. This chapter argues that thinking about symmetric games gives us new reason to believe in permissivism. In some finite games, if permissivism is false then we have to think that a player is more likely to take one option rather than another, even though each have the same expected return given that player’s credences. And in some infinite games, if permissivism is false there is no rational way to play the game, although intuitively the games could be rationally played. The latter set of arguments rely on the recent discovery that there are symmetric games with only asymmetric equilibria. It was long known that there are symmetric games with no pure strategy symmetric equilibria; the surprising new discovery is that there are symmetric games with asymmetric equilibria, but no symmetric equilibria involving either mixed or pure strategies. The permissivist theses that have been the focus of recent philosophical attention vary along two dimensions.11 The first dimension concerns what we hold fixed when we say that multiple attitudes are rationally permissible. The weakest possible theory just says that two people with distinct attitudes may both be rational. No one really denies this. The strongest theory says that holding every fact about a situation constant, there are two possible rational attitudes. In between we have a number of interesting theses. For instance, we can ask whether multiple attitudes are rationally compatible holding constant the evidence the believer has. And we can ask whether multiple attitudes are rationally compatible holding constant both the evidence and the believer’s prior doxastic states. A classic form of subjective Bayesianism answers yes to the first question, and no to the second. The focus here will be on a thesis very close to the strongest one - whether two people who are alike in all qualitative respects can rationally have different attitudes. The second dimension concerns whether the folks holding these distinct attitudes can acknowledge that rival attitudes are rational. Some permissivists hold that distinct attitudes can be rationally compatible with holding fixed evidence or priors or whatever, but the people holding these attitudes cannot acknowledge that attitudes other than theirs are rational. The argument I’m going to offer draws the stronger conclusion that multiple responses are rational, and rational thinkers can acknowledge that alternative responses to theirs are rational. The negation of a permissive thesis is a Uniqueness thesis. The name suggests that there is precisely one rational attitude to take in a specified situation, but we’ll interpret it as the view that there is at most one rational attitude to take so as to ensure each Uniqueness thesis is the negation of a permissive thesis. As with Permissivism, Uniqueness comes in weaker and stronger varieties. The strongest version is the literally incredible view that there is only one doxastic attitude that is rationally permissible. (Presumably it is the view that is certain of all and only truths.) The weakest version, which is still interesting, is that once a situation is described in full detail, there is precisely one doxastic attitude that is rationally permissible. Everyone holds that, since the normative supervenes on the descriptive, that describing a situation in full detail fixes which doxastic states are rationally permissible. The Uniqueness theorist adds the claim that there are 0 or 1 such states.12 As I said, I’m interested in defending a strong, but not maximally strong, version of Permissivism. Equivalently, I’m interested in attacking a weak, but not quite maximally weak, version of Uniqueness. Here is the version of Uniqueness that I want to reject. For all kinds \\(K\\), and evidence \\(E\\), there is at most one credal distribution that is rational for an agent of kind \\(K\\) with evidence \\(E\\). I mean to be fairly liberal over what counts as a `kind’, so if any such theory is false, we have proven that a lot of Uniqueness theories are false. So a kind could be a prior probability function, a set of privileged predicates that one uses for induction, an attitude to inductive risk, and so on. The only assumption I’ll make is that kinds are shareable; so there is no such thing as the kind Being John Malkovich. In principle we could say that what evidence one has is part of one’s kind, but the discussion below will be clearer if we separate out evidence and kinds. The next two sections set out two symmetric games where Uniqueness leads to surprising results. I think the results are surprising, indeed implausible, enough that we should reject Uniqueness. But even if one doesn’t accept that, it’s still interesting to see what Uniqueness entails. In all cases we’ll assume that the following things are common knowledge among players of the game. Each player is rational, so they form rational credences, and maximise expected utility. Each player is of kind \\(K\\). Each player knows the payout structure of the game. Each player is self-aware; they know their own credences. If Uniqueness is true, then each player knows that Uniqueness is true. Each player has no other relevant evidence about the game or the players. Let evidence \\(E\\), unless otherwise stated, be the evidence specified by those six bullet points. We’ll be continually thinking about propositions of the form: A rational agent of kind \\(K\\) with evidence \\(E\\) will perform action \\(varphi\\) in this game. Since the games are symmetric, we don’t have to ask about which player will make this move; we can think abstractly about what any rational player would do. 8.2 Chicken Some finite symmetric games have no symmetric pure-strategy equilibria. One notable example is Chicken. Table 8.1 shows a version of Chicken that will do. Table 8.1: Chicken Stay Swerve Stay -100, -100 1, -1 2 1, -1 0, 0 The symmetric pure-strategy pairs \\(\\langle\\)Stay, Stay\\(\\rangle\\) and \\(\\langle\\)Swerve, Swerve\\(\\rangle\\) are not equilibria; in each case both parties have an incentive to defect. But the game does have a symmetric equilibria. It is that both players play the mixed strategy of Stay with probability 0.01, and Swerve with probability 0.99. Let Swerve be the proposition that a rational player of kind \\(K\\) with evidence \\(E\\) will Swerve. And call the players Row and Column. Given our assumptions so far, plus Uniqueness, we can prove that Row’s credence in Swerve is 0.99. Here’s the proof. Let \\(x\\) be Row’s credence in Swerve. By self-awareness, Row knows that \\(x\\) is her credence in Swerve. Since she knows she is rational, Row can infer that \\(x\\) is a rational credence in Swerve. Since she knows Uniqueness is true, Row can infer that \\(x\\) is the only rational credence in Swerve. Since she knows Column is rational, she can infer that \\(x\\) is Column’s credence in Swerve. Since all the assumptions so far are common knowledge, she can infer that Column knows that \\(x\\) is her credence in Swerve. If \\(x = 1\\), then Row can infer that it is rational for Column to Swerve, while knowing that Row will also Swerve. But this is impossible, since if Column knows Row will Swerve, it is best to Stay. So \\(x \\neq 1\\). If \\(x = 0\\), then Row can infer that it is rational for Column to Stay, while knowing that Row will also Stay. But this is impossible, since if Column knows Row will Stay, it is best to Swerve. So \\(x \\neq 0\\). So \\(0 &lt; x &lt; 1\\). Since Row knows Column’s credence that Row will Swerve (whatever it is), and Row knows Column is rational, but Row does not know what Column will do, it must be that Column is indifferent between Stay and Swerve given her credences about what Row will do. Column is indifferent between Stay and Swerve only if her credence that Row will Swerve is 0.99. (This is a reasonably simple bit of algebra to prove.) So from 10 and 11, Column’s credence that Row will Swerve is 0.99. By (known) Uniqueness, it follows that the only rational credence in Swerve is 0.99. So since Row is rational, it follows that \\(x = 0.99\\). Now there is nothing inconsistent in this reasoning. In a sense, it is purely textbook reasoning. But the conclusion is deeply puzzling. We’ve proven that Column is indifferent between her two options. And we’ve proven that Row knows this. But we’ve also proven that Row thinks it is 99 times more likely that Column will choose one of the options over the other. Why is that? It isn’t because there is more reason to do one than the other; given Column’s attitudes, the options are equally balanced. It is purely because Uniqueness pushes us to a symmetric equilibrium, and this is the only symmetric equilibrium. I don’t think this result will convince many devotees of Uniqueness to give up their view. It’s not a particularly novel claim that rational players will end up at the unique Nash equilibrium of a game. And to be sure, if this game was being played repeatedly, much weaker assumptions entail that each player should Stay 1% of the time, with those Stays being randomly distributed across the plays of the game. But it is still odd, at least to me, to see the same conclusion drawn in the single shot game, where each player is known to be indifferent between their choices. The next case is I think much worse for Uniqueness. 8.3 Elections The cases that really inspired this chapter come from some recent work on this rather old question, If a symmetric game has an equilibrium, does it have a symmetric equilibrium? Over the years, a positive answer was given to various restricted forms of that question. Most importantly, John Nash (1951) showed that if each player has finitely many moves available, then the game does have a symmetric equilibrium. But recently it has been proven that the answer to the general question is no. Mark Fey (2012) showed that there are symmetric positive-sum two-player games that have only asymmetric equilibria.13 And Dimitrios Xefteris (2015) showed that there is a symmetric three-player zero-sum that has only asymmetric equilibria. In fact, he showed that a very familiar game, a version of a Hotelling–Downs model of elections, has this property. Here’s how he describes the game. Consider a unit mass of voters. Each voter is characterized by her ideal policy. We assume that the ideal policies of the voters are uniformly distributed in [0, 1]. We moreover assume that three candidates A, B and C compete for a single office. Each candidate \\(J \\in \\{A, B, C\\}\\) announces a policy \\(s_J \\in [0, 1]\\) and each voter votes for the candidate who announced the policy platform which is nearest to her ideal policy. If a voter is indifferent between two or among all three candidates she evenly splits her vote between/among them. A candidate \\(J \\in \\{A, B, C\\}\\) gets a payoff equal to one if she receives a vote-share strictly larger than the vote-share of each of the two other candidates. If two candidates tie in the first place each gets a payoff equal to one half. If all three candidates receive the same vote-shares then each gets a payoff equal to one third. In all other cases a candidate gets a payoff equal to zero. (Xefteris 2015, 124) It is clear that there is no symmetric pure-strategy equilibrium here. If all candidates announced the same policy, everyone would get a payoff of \\(\\frac{1}{3}\\). But no matter what that strategy is, if B and C announce the same policy, then A has a winning move available. What’s more surprising, and what Xefteris proves, is that there is no symmetric mixed strategy equilibria either. Again, in such an equilibrium, any player would have a payoff of \\(\\frac{1}{3}\\). Very roughly, the proof that no such equilibrium exists is that random deviations from the equilibrium are as likely to lead to winning as losing, so they have a payoff of roughly \\(\\frac{1}{2}\\). So there is no incentive to stay in equilibrium. So no symmetric equilibrium exists. But if Uniqueness is true, and if it is possible to play the game under circumstances of common knowledge of rationality and kind, then there must be a symmetric equilibrium. The reason is that a version of the proof of the previous section still goes through. Whatever credal distribution A has over B’s possible policies, A must also have over C’s policies (since they both adopt the uniquely rational strategy), and she must know that B and C each have over each other’s policies and over hers, and these distributions must be consistent with each player having these credal distributions while thinking that the other players have the same distributions and are maximising expected utility. In other words, the assumptions we’ve made about the game imply that A has a credal distribution \\(F\\) over B’s possible policies only if the mixed strategy triple where each player adopts \\(F\\) as their mixed strategy is itself an equilibrium. And that would be a symmetric equilibrium. But no symmetric equilibrium exists. But if we drop Uniqueness, it is possible to keep all the other assumptions. As Xefteris points out, the game has asymmetric equilibria. Here is one possible model for the game. A plays 0.6 (and wins), B and C each play 0.4 (and lose). Each player has a correct belief about what the other players will play. But both B and C know they cannot win given the other player’s moves, so they pick a move completely arbitrarily. Further, each player has a correct belief about why each player makes the move they make. This is the coherent equilibria that Xefteris describes, but note that it is rather implausible that we’d end up there in a real-life version of the game. It requires two of the players to know that one of the other players will be indifferent between their options, but from this draw a correct inference about what they will do. That’s not particularly plausible. So let’s note that there is a somewhat more plausible way to get all three players to make those moves. A plays 0.6 (and wins), B and C each play 0.4 (and lose). The only two rational plays are 0.4 and 0.6, but each of them is permissible. In any world that a player believes to be actual, or a player believes another player believes to be actual, or a player believes another player believes another player believes to be actual, etc., the following two conditions hold. If a player plays 0.6, they believe the other two players will play 0.4, and hence playing 0.6 is a winning move. If a player plays 0.4, they believe the other two players will play 0.6, and hence playing 0.4 is a winning move. The main difference between this model and Xefteris’s is that it allows that players have false beliefs. But why shouldn’t they have false beliefs? All they know is that the other players are rational, and rationality (we’re assuming) does not settle a unique verdict for what players will do.14 So I think this strategy set, where the players have rational (but false) beliefs about the other players, is more useful to think about. And what it shows, I think, is that Uniqueness implies a false claim about which games are possible. Given Uniqueness, it is impossible for players who know each other to be rational to play this game. But it is possible for players who know each other to be rational to play this game; we’ve just seen a way the world could be where both players know the other to be rational. So Uniqueness is false - it implies the impossibility of something we’ve seen to be possible. 8.4 Objections The reductio arguments in the previous two sections have assumed not just that Uniqueness is true, but that the players know that it is true. What happens if we drop that assumption, and consider the possibility that Uniqueness is true but unknowable? This possibility is a little uncomfortable for philosophical defenders of Uniqueness. If the players in these games do not know that Uniqueness is true, then neither do the authors writing about Uniqueness. And now we have to worry about whether it is permissible to assert in print that Uniqueness is true. I wouldn’t make too much of this though. It is unlikely that a knowledge norm governs assertion in philosophical journals. The bigger worry here is that one key argument for Uniqueness seems to require that Uniqueness is knowable. A number of recent authors have argued that Uniqueness best explains our practice of deferring to rational people.15 For instance, Greco and Hedden use this principle in their argument for Uniqueness. If agent \\(S_1\\) judges that \\(S_2\\)’s belief that \\(P\\) is rational and that \\(S_1\\) does not have relevant evidence that \\(S_2\\) lacks, then \\(S_1\\) defers to \\(S_2\\)’s belief that \\(P\\). (Greco and Hedden 2016, 373). Similar kinds of arguments are made by Dogramaci (2012) and Horowitz (2014). But the principle looks rather dubious in the case of these games. Imagine that A forms a belief that B believes that a rational thing to do in the Xefteris game is to play 0.6, and so she will play 0.6. She should judge that belief to be rational; as we saw it is fully defensible. But although she does not believe that she has evidence that B lacks, she should not defer to it. At least, she should not act as if she defers to it; believing that B will play 0.6 is a reason to play something other than 0.6. And that’s the general case for these symmetric games with only asymmetric equilibria. Believing that someone else is at an equilibrium point is a reason to not copy them. If it were not a reason to not copy them, then the strategy profile where each player plays the same thing would be a symmetric equilibrium. So thinking about these games doesn’t just give us a rebutting defeater for Uniqueness, as described in the previous two sections, but an undercutting defeater, since they also tell against a premise that has been central to recent defences of Uniqueness. I think there is a somewhat better move available to the Uniqueness theorist. They could simply deny that the Xefteris game, as I’ve described it, is even possible.16 This perhaps isn’t as surprising as it might seem. Note two things about the Xefteris game. First, it is an infinite game in the sense that each player has infinitely many choices. It turns out this matters to the proof that there is no symmetric equilibrium to the game. Second, we are assuming it is common knowledge, and hence true, that the players are perfectly rational. Third, we are assuming that perfect rationality entails that people will not choose one option when there is a better option available. When you put those three things together, some things that do not look obviously inconsistent turn out to be impossible. Here’s one example of that. A and B are playing a game. Each picks a real number in the open interval (0, 1). They each receive a payoff equal to the average of the two numbers picked. For any number that either player picks, there is a better option available. It is always better to pick \\(\\frac{x+1}{2}\\) than \\(x\\), for example. So it is impossible that each player knows the other is rational, and that rationality means never picking one option when a better option is available. So the Uniqueness theorist could say that the same thing is going on in the Xefteris game. Some infinitely games cannot be played by rational actors (understood as people who never choose sub-optimal options); this is one of them. But if this is all the Uniqueness theorist says, it is not a well motivated response. We can say why it is impossible to rationally play games like the open interval game; the options get better without end. But that isn’t true in the Xefteris game. The only thing that makes the game seem impossible is the Uniqueness assumption. People who reject Uniqueness can easily describe how the Xefteris game can be played by rational players. Simply saying that it is impossible, without any motivation or explanation for this other than Uniqueness itself, feels like an implausible move. 8.5 Conclusion If Uniqueness is true, then the following thing happens in games between people who know each other to be the same kind, and to be rational. When someone forms a belief about what the other person will do, they can infer that this is a rational way to play the game given knowledge that everyone else will do the same thing. But sometimes this is a very unintuitive inference. In Chicken, it implies that we should have asymmetric attitudes to someone who is facing a choice between two options with equal expected value. In the election game Xefteris describes, a game that feels consistent turns out to be impossible. I think the conclusion to draw from these cases of symmetric interactions this is that Uniqueness is false, and hence permissivism is true. Sometimes in such an interaction one simply has to form a belief about the other player, knowing they may well form a different belief about you. Indeed, sometimes only coherent way to form a belief about the other player is to believe that they will form a different belief about you. And that means giving up on Uniqueness. And if we give up on Uniqueness, then there is no tension between the views in chapters 4 and 6. For a much more thorough introduction to the debate, and especially into the varieties of permissivist theses, see Kopec and Titelbaum (2016). Much of the setup here, including for example the use of the subjective Bayesian as an illustrative example, is from that paper. Some notable more recent contributions to the debate include Schultheis (2018) arguing for Uniqueness, and Callahan (2021) arguing for Permissivism. Callahan connects Permissivism to existentialism. I suspect there are deep and unexplored connections here, but exploring them is a task for a different book.↩︎ Two small caveats here. Uniqueness theorists may say that it is permissible to not have any attitude towards a proposition. So it is consistent with Uniqueness, as I understand it, to say that it could be both rational to have credence 0.6 in \\(p\\), and rational to not have any attitude towards \\(p\\). What the Uniqueness theorist denies is that there are distinct credences towards \\(p\\) one could adopt, each of which would be rational. And of course the Uniqueness theorist thinks it could be rational to have credence 0.6 in \\(p\\) and 0.7 in \\(q\\). When I say Uniqueness implies that just one state is rational, I mean to quantify over complete credal states, not attitudes towards single propositions.↩︎ Fey also includes a nice chronology of some of the proofs of positive answers to restricted forms of the question.↩︎ To use the game-theory jargon, Xefteris describes a Nash equilibrium of the game, but what I’ve described is a a rationalizable strategy triple (Bernheim 1984, Pearce1984). If Uniqueness is true, then strictly speaking any rationalizable strategy pair for a symmetric game is a Nash equilibrium.↩︎ There is a nice discussion of this argument, including citations of the papers I’m about to discuss, in Kopec and Titelbaum (2016, 195).↩︎ This is really just a response to the argument based on that game; I think they just have to say that in Chicken a rational player will rationally think the other player is more likely to make one of the two choices with equal expected payoffs.↩︎ "],["puzzles-about-weak-dominance.html", "Chapter 9 Puzzles about Weak Dominance 9.1 Why Weak Dominance 9.2 The Boundaries of Games 9.3 Three Kinds of Demon 9.4 Benefits of Weak Dominance 9.5 Iterated Weak Dominance", " Chapter 9 Puzzles about Weak Dominance The account causal ratificationism gives of what makes a decision rational has two clauses. The first is that the decision has to make sense once it is made. That is, it has to maximize expected utility given some credal distribution that is rational once the decision is made. The second is that the decision must not be weakly dominated. That is, given the states and choices that are possible (given that the decision is made), no other option must have the status that it could do better, and couldn’t do worse. The first clause seems very intuitive to me, and is backed up by plausible principles about choices like the ABC game. The second clause seems less intuitive. But it, or something like it, is required to make sense of those same plausible principles, and so I’ve included it as part of causal ratificationism. This chapter has four aims, corresponding to its four sections. In section 9.1 I’ll show why the ABC game supports the idea that rational choosers do not select weakly dominated options. In section 9.2 I’ll note one reason that weak dominance unintuitive; it suggests that seemingly insignificant boundaries are in fact significant. If it weren’t for the argument of the previous section, this would be enough to convince me to do without weak dominance. In section 9.3 I’ll discuss how weak dominance interacts with some hard puzzles about a simple symmetric game. I’ve had two previous attempts to say something sensible about this game [(Weatherson201x?); Weatherson201y]; hopefully third time’s the charm. Finally, in section 9.5, I’ll argue that it can be rational to choose options which are ruled out by iterated weak dominance. The argument will primarily turn on an analysis of the money-burning game [cite], and I’m going to endorse Stalnaker’s [cite] criticism of the standard treatment of that game. 9.1 Why Weak Dominance Consider the following version of the ABC game. Figure 9.1: A version of the ABC game that supports weak dominance. GOTTA CLARIFY WHETHER I’M REALLY ALREADY COMMITTED TO THIS ALSO RAISE WORRY ABOUT WHETHER WEAK DOMINANCE LEADS TO PARADOX IN VERSIONS OF ABC WHERE DEMON HAS PR 1 IN A Given what I’ve said already about the ABC game, I’m committed to the following two claims. First, in this game the only rational choice is \\(U\\). That’s the only rational choice at the only information set where Chooser might move, and that determines the rational strategy. Second, a choice is rational in this game iff it is rational in the following simultaneous move game. Table 9.1: A strategic representation of this ABC game. A B C U \\(2, 2\\) \\(1, 1\\) \\(1, 0\\) D \\(2, 2\\) \\(0, 0\\) \\(0, 1\\) From these claims, it follows that the only rational move in the game in @(tab:abc-weak-dominance-strategic) is \\(U\\). But that’s hard for the causal ratificationist to explain, since it seems that \\(D\\) is ratifiable. If Chooser believes, after choosing \\(D\\), that the probability that Demon will play \\(A\\) is 1, then playing \\(D\\) will maximise expected utility. Is that a rational credence to have? Maybe! After all, Chooser knows that Demon is a utility maximiser, and for Demon, \\(A\\) strictly dominates \\(B\\) and \\(C\\). So if causal ratificationism was just restricted to the view that choices must be ratifiable, it would not be consistent with what I’ve said about the ABC game, since it would say that \\(D\\) is impermissible in the extensive form version of the game, but permissible in the strategic form. This shows that I need to add something to causal ratificationism to make it consistent with what I’ve said about the ABC game. Adding weak dominance would solve this problem. While \\(D\\) is ratifiable in the strategic form of the game, it is weakly dominated. So adding the weak dominance clause restores the key constraint that the two forms of the game are treated the same way. And that’s the primary reason that I added this clause to causal ratificationism. Note that this argument is well short of a proof that weak dominance is needed. It shows something is needed, and weak dominance would be sufficient, but it doesn’t rule out weaker constraints. But I can’t see any weaker constraint that would suffice to bring these two problems back into alignment, and would be nearly as intuitive as weak dominance. So I’ve added it as the second clause. There is one intuitive motivation for weak dominance. The core idea behind causal ratificationism is that choices should be defensible. The chooser should be able to say “This is what I’m doing, and this is why I’m doing it,” in a way that makes sense. And there is something strange about a speech defending choosing \\(D\\) in @(tab:abc-weak-dominance-strategic). It looks like the person who chooses \\(D\\) is taking a risk for which they are not receiving any compensation. And it’s irrational to take uncompensated risks. That intuition, if it is correct, generalises to all cases of choosing weakly dominated options, and so supports a general bar on choosing weakly dominated strategies. Weak dominance has some problems though, and I’ll turn to them next. 9.2 The Boundaries of Games The definition of weak dominance involves quantifying over possible ways the world might be. In the cases that have been most central to this book, those states have been moves made by another agent, typically a demon. So we could say, equivalently, that the definition of weak dominance involves quantifying over moves available to the other player. Indeed, the usual definition of weak dominance in game theory does quantify over moves available to the other player. A consequence of that is that the boundary between the available and the unavailable moves becomes very significant. That’s a problem, because often this boundary seems to be insignificant. The importance of this boundary is a key difference between the two clauses in causal ratificationism. The definition of expected utility maximization also makes an appeal to the boundary between possible and impossible states. But whether one treats a state as impossible, or as possible but with probability zero, doesn’t affect the expected utility of an action. The difference between treating a state as impossible, and as possible but with probability zero, does matter to determining which states are weakly dominant. It’s easier to think about this boundary with some concrete examples. So let’s get away from the simple games that have been the focus of this book, and imagine that Chooser is playing chess with Demon. In this game, Chooser has the black pieces. The game starts in a fairly traditional manner: 1. e4 e5, 2. Nf3. Now it is Chooser’s turn to move, and they are thinking about Qh4. This is obviously a bad move, but I want us to think for a minute about why it is a bad move. The causal ratificationist says that when evaluating a move, one thing to check is whether it is weakly dominated. To do that, one must know what the possible states are, i.e., what the possible moves for Demon are. Now pretty clearly Qh4 is not going to be utility maximizing, so the answer to this question won’t affect what the causal ratificationist ultimately says about whether one should play Qh4. But it is important to figure out just what the theory says, so it is important to figure out just what these ‘possible moves’ are. In particular, which of the following two ‘moves’ should be in the domain of quantification when we are asking whether a choice by Chooser is weakly dominated? Demon moves Ng5. Demon knocks over the board and walks away. There is an important sense in which 1 is in the domain, and 2 is not. The rules of chess allow for Ng5, and they do not allow for knocking the board over. When I appealed to weak dominance earlier, I assumed that all and only the moves specified in the rules of the game went into the domain of quantification, and I suspect most readers went along with that. The same principle here would include Ng5, and exclude knocking the board over, as possible moves. This division is hard to motivate from a traditional philosophical perspective. There is no relevant epistemic difference between the two options. Given very weak assumptions, Chooser knows that Demon will take neither of these options. There is no relevant difference between the options in terms of ability. Demon can move Ng5, and can also (given some assumptions about Demon’s corporal form) knock the board over. The difference between the options is essentially a legal one. Ng5 is a legal move, and knocking the board over is not. That matters when one is playing a formal game. But game theory isn’t just meant to be about formal games like chess. It is meant to include human interactions, notably including wars, that don’t have these kinds of clear rules. And in those settings, it is unclear what counts as a possible move, and hence unclear what counts as a weakly dominant move. Let’s bring this all back to decision theory involving demons. Chooser is playing the following game. They have to choose \\(U\\) or \\(D\\); if they choose something else, they get nothing. Demon is very good at predicting them, and will either turn to the left, if they predict \\(U\\), or turn to the right, if they predict \\(D\\). The turn will be after Chooser writes down their choice, but before it is revealed. If Chooser writes \\(U\\), they get 1 whichever way Demon chooses, but if Chooser writes \\(D\\), they get 1 if Demon turns right, and 0 if Demon turns left. So far, this looks like a case where weak dominance matters. The expected return of each move is 1, but \\(U\\) weakly dominates \\(D\\). Let’s add something to the game though. Demon will turn one way or the other; Chooser knows that. But if Demon instead drinks a beer, Chooser will get 1 if they have chosen \\(D\\), and 0 if they have chosen \\(U\\). Demon won’t drink a beer; Demon is allergic to beer, and there isn’t any beer around. But these are the payoffs if beer drinking happens. Which it won’t. Question: Does \\(U\\) still weakly dominate \\(D\\)? And this suggests a further question: Is \\(D\\) permissible? The problem for using weak dominance in a theory of choice, and it’s a problem wherever weak dominance is used, not just in causal ratificationism, is this. Given that Demon definitely won’t drink beer, it seems like it shouldn’t matter whether we exclude beer from the decision table, or include it while marking it as maximally unlikely to happen. But given that weak dominance is one of our constraints, it turns out this makes all the difference in the world. That seems unfortunate. Here is how I think we should deal with this problem. I’ve said deal with rather than solve advisedly. It is a problem; it’s why I’m not happy having to include a weak dominance clause. But there are ways to deal with it. In the first instance, causal ratificationism is a theory of decision not in real world decision problems, but in the kind of formalized problems we discuss in decision theory and game theory textbooks. Solving a real world problem is a two-step process. First, the real world problem has to be translated into a formal problem. Causal ratificationism has something to say about this—it says that the states must be causally independent of the options—but it doesn’t have a lot to say. Second, the formal problem has to be solved. Causal ratificationism has a lot to say about this stage, and I’ve been spelling out what it has to say at some length in this book. Usually, there are multiple acceptable ways to translate a real world problem into a formal problem. Sometimes, the different translations will have different solutions. In those cases, it is indeterminate what is rational to do. Arguably, that’s the case in the case I described a few paragraphs back, about turning and beer-drinking. It’s acceptable to model this as a problem where Demon’s only options are to turn left or to turn right. On that model, the only rational move is \\(U\\). It’s also acceptable to model it as a problem where Demon could (but won’t) drink beer. On that model, both \\(U\\) and \\(D\\) are rationally permissible. So it is determinately true that \\(U\\) is rationally permissible, but indeterminate whether \\(D\\) is rationally permissible. That is a plausible enough response to the problem, I think. I said earlier that to solve a real world problem, one has to do two things: translate the problem into a formal problem, and solve the formal problem. There is a way that might be misleading. It might suggest that the two steps are analytically distinct from each other, and that the plausibility of an answer to one problem should be assessed independently of the answer to the other. That’s not right. Whether a formal model of a real world problem is acceptable is a function, in part, of whether the solutions to the formal problem are reasonable solutions to the real world problem. It’s perfectly sensible to reason as follows. If it is clear that \\(D\\) is an unreasonable move in the real world game (I don’t think this is clear, but someone might), then it follows that modeling the game as a formal game where Demon has three possible moves—turn left, turn right, drink beer—is to model the game incorrectly. There is no such thing as an absolutely correct or incorrect model of a game, or of any other real world situation; the quality of a model is dependent on what the model is being used for. And this also helps us avoid some of the most troublesome consequences of the fact that weak dominance is dependent on just how a game is modeled. Sometimes, the fact that rational choices are not weakly dominated is evidence for or against modeling the game a particular way. 9.3 Three Kinds of Demon Set up red green game and properly cite Note this is my third try at it First demon, limit prob. This one is easy Second demon, zero prob but possible. Answer one this can’t happen. But this takes us into infinitesimal territory and I’m not going there. Answer two, the speech sounds bad. Don’t take uncompensated risks. Third demon, can not fail. Then not in fact weak dominance post choice bc alternative is not in fact possible. Same goes for symmetric humans but third is really impossible. 9.4 Benefits of Weak Dominance Chooser and Demon pick an integer between 0 and 1,000,000. Demon gets 1 if same, 0 if otherwise, and Demon is very good Chooser gets lower of two numbers unless they pick the same positive number, in which case Chooser gets that minus two. Only ratifiable outcome is 0,0 But that’s weakly dominated by just about everything else, and so should be rejected. Weak dominance helps here. Question: What equilibria are there? Feels like there should be a mixed strategy one. Note to self: Think about case where game is capped at 10. Is there an equilibrium where Chooser plays 1/2 9, 1/2 10. Need expected return of 9 and 10 to be the same. Demon plays 9 with prob x, 10 with prob 1-x. If Chooser plays 9, gets 7x + 10(1-x) = 10 - 3x. If Chooser plays 10, gets 9x + 8(1-x) = 8 + x Haha, it’s 50/50. Another sort of benefit. Think about the following three option case. PU PM PD U 0 5 2 M 5 0 2 D 2 2 2 Is D acceptable? No, I say, it’s weakly dominated by 1/2 U, 1/2 M. That’s good enough. Ideally, Chooser would play that. Even if they can’t play it, they should. The same helps if you take the earlier problem, and say that if 0, 0, Chooser gets payout epsilon. It’s still weakly dominated by this mixture. Oh that’s not right - Hmm come back to this. 9.5 Iterated Weak Dominance Sometimes thought that if WD then committed to IWD. Cite HH and Stalnaker in reply. Three objections 1. Order effects 2. Gets unintuitive results 3. Nothing incoherent about speeches that violate Start with Bonanno on order effects Do strategic version of money burning and show what IWD leads to Note that nothing wrong with all H speech This might get complicated Now do dynamic version Really absurd that having an untaken option can make a difference, when others know you won’t take it But does HH mean that you think demon is stupid? No it means that you think demon might follow up stupid with stupid Also do this using counterfactuals maybe "],["gametheory.html", "A Game Theory", " A Game Theory This appendix is a brief introduction to some important game theoretic concepts. It won’t introduce anything new to anyone who has studied practically any game theory before. But since that’s not true of all philosophers, I thought it was helpful to briefly introduce the basics here. "],["aboutademon.html", "B About a Demon", " B About a Demon Include stuff about arbitrarily accurate Note that known perfect accuracy leads to challenges, one’s we’ll come back to in… Also note that we’re assuming the demon predicts the strategy a player chooses. That could include randomisation. Note that I’m simply assuming expected utility theory is right for non-demonic problems. But that does imply an assumption that there is some reason for that. And I think that’s got to include something like a Sure Thing Principle. But I am including the open box Newcomb Problem as demonic. And maybe a riff on why it’s Player/Chooser/Agent whatever "],["methodology.html", "C Methodology", " C Methodology There are three primary sources of evidence that are available in constructing and evaluating decision theories: principles, sameness of cases, and cases. Most of the literature focusses on the third of these. I think that’s a mistake; this is by far the least reliable source of evidence. We should instead focus on the first two. That’s what I’ll do in this book, and the purpose of this section is to defend this methodological starting point. By principles, I mean claims like Preferences should be transitive, or Choosers should not regret their choices as soon as they are made. These are very general claims about what the structure of one’s choices should look like. There aren’t a lot of principles like these that we can be antecedently very confident in. But there are some, and those we should hold on to barring extraordinary evidence. By sameness of cases, I mean claims that two particular decisions should get (in some sense), the same choice. Here is a familiar example from a non-Demonic decision problem. Chooser has a ticket to today’s cricket match, and is deciding whether to go. Chooser enjoys watching cricket, but does not enjoy sitting around in the stands waiting for the rain to clear, and there is a good chance of rain. What should Chooser do? Well, we haven’t said nearly enough to settle that, so let’s ask something more precise. What more do we need to know to know what Chooser should do? We do need to know how much Chooser likes watching cricket, dislikes sitting around in the rain, will have to pay to get to the ground and how likely rain is. But we don’t need to know how much Chooser paid for the ticket. That’s a sunk cost. If we settle all the forward looking parameters (likelihood of rain, utility of going under different scenarios, etc), then changing the backwards looking ones (how much Chooser paid) doesn’t make a difference. If it would be rational to stay home given all those parameters and having paid $10 for the ticket, it would be rational to stay home given all those parameters and having paid $100 for the ticket. Even in hard cases, and if the weather is bad enough Chooser may have a very hard choice here, we often have clear enough knowledge that some differences do not in fact make a difference. And by cases I mean claims about what Chooser should do in a particular vignette. These are the main form of evidence that get used in philosophical decision theory. We (the theorists) are told that Chooser faces a problem like the one in C.1, with a Demon predicting the choice and the payout dependent on the actions of the two of them. Table C.1: An example of a choice situation PA PB A \\(6\\) \\(0\\) B \\(4\\) \\(3\\) A lot of philosophers seem to the following approach to decision theory. (I say ‘seem to’ because I’ve never really seen a good defence of this methodology, but I have seen a lot of arguments from cases like this to sweeping theoretical claims.) First, we figure out what Chooser should do in this case, perhaps by consulting our intuitions. Second, we work out which theory best fits with what we’ve learned this way about individual cases. Now it would be wrong to say that the three kinds of evidence I’ve presented here fall into three very neat categories. The boundaries between them are blurry at best. Any evidence about an individual case can be turned into a kind of principle by simply replacing the names in the vignette with variables and universally quantifying over the variables. We’ll get very restricted principles that way—anyone facing just that game should play B, for example—but all principles have some restrictions on them. And there’s not much distance between judging that what Chooser should do is independent of what they paid for the ticket, and judging that the principle Sunk costs are irrelevant is part of our evidence. But the fact that a boundary isn’t sharp doesn’t mean that it’s theoretically useless. The paradigms of the three kinds of evidence are different enough that we can helpfully keep them in mind when understanding what particular theorists are doing. And the paradigm that starts with individual cases, like the table I just presented, seems considerably worse than the other two paradigms. Arguments in that family are vulnerable to two kinds of objection that arguments that start with the other kinds of evidence are not. First, judgments about cases might conflate what the standards of ideal rationality say about the case with what the standards of non-ideal rationality say about the case. Consider this example, which I discuss more in chapter 5. Quick Basketball Choice Chooser has to name an NBA team. (The NBA is the main club basketball competition in the world.) If the team wins the NBA championship this year, Chooser wins a million dollars. But if Chooser thinks about any basketball player before naming the team, Chooser will be cast into the fires of Hell for all of eternity. What should Chooser do? I think Chooser should say the first team name that comes into their head, or just pass and get out of the game if that’s possible. They should very much not try to make the choice that maximises expected utility. That will require forming probabilistic judgments about the likelihood of different teams winning, and that will involve thinking about the players on the team, and that will involve damnation. Just say something and hope for the best. Is this a counterexample to the claim that in non-Demonic cases Chooser should maximise expected utility? No, because that claim is about ideal rationality. When there are constraints on how Chooser can make the choice, ideal rationality doesn’t apply. Chooser should be judged by standards of non-ideal rationality, and non-ideal rationality says to say the first team name that comes into his head. I’ll make all this a fair bit more precise in chapter 5, including coming back to whether it makes sense to talk as if there is a single standard of non-ideal rationality. But I hope it’s reasonably clear that something like this is the right thing to say. There are multiple ways we can judge individual choices in individual cases. The standards of ideal rationality are one of those ways to judge cases, but not the only way. And sometimes when we ask what someone should do, we are (implicitly) using one of those other ways. I think most purported counterexamples to one or other decision theory in the literature are no more convincing than using Quick Basketball Choice to refute expected utility theory. This is especially true when they involve, as this case does, constraints not just on what decision is made, but how it is made. (For example, when they involve the decision maker being punished for randomising). And this is one reason why I distrust arguments from particular examples. The second reason is more internal to decision theory. The project I’m engaged in, and the project that most philosophers who write about demonic decision theory are engaged in, is the project of generalising expected utility theory to cover Demonic cases. But the methodology of starting with judgments about individual decisions and finding the theory that best fits them does not, in fact, lead to expected utility theory. So that methodology is inconsistent with the theory we are assuming to be correct in non-Demonic cases. So there is a fairly deep tension in any work that tries to generalise expected utility theory by appeal to intuitions about cases. This point requires a bit of background to make, so let’s come back to it after saying more precisely what the theory that we’re all trying to generalise is. "],["tables.html", "D Decision Tables and Decision Problems", " D Decision Tables and Decision Problems Five questions What is a permissible state? When is it permissible to end the list of states? What is a permissible option? When is it permissible to end the list of options? What do the values measure? And a sixth question, what does it take to individuate a problem. "],["quiggin.html", "E Non-Demonic Decision Theory", " E Non-Demonic Decision Theory "],["backwardinduction.html", "F Backward Induction", " F Backward Induction What I’m using Joyce’s review of Ahmed for why it matters The strong version I’m not using (or maybe that I am using) Stalnaker on why it shouldn’t be used "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
